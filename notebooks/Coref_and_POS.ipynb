{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dmac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/dmac/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f853c8c2610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp, blacklist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\", return_dict=True)\n",
    "import numpy as np\n",
    "emotions = ['admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity',\\\n",
    "            'desire','disappointment','disapproval','disgust','embarrassment','excitement','fear',\\\n",
    "            'gratitude','grief','joy','love','nervousness','optimism','pride','realization','relief',\\\n",
    "            'remorse','sadness','surprise','neutral']\n",
    "reduced_emotions = {\n",
    "    'admiration' : 'pos',\n",
    " 'amusement' : 'pos',\n",
    " 'anger' : 'neg',\n",
    " 'annoyance' : 'neg',\n",
    " 'approval' : 'pos',\n",
    " 'caring' : 'pos',\n",
    " 'confusion' : 'amb',\n",
    " 'curiosity' : 'amb',\n",
    " 'desire' : 'pos',\n",
    " 'disappointment' : 'neg',\n",
    " 'disapproval' : 'neg',\n",
    " 'disgust' : 'neg',\n",
    " 'embarrassment' : 'neg',\n",
    " 'excitement' : 'pos',\n",
    " 'fear' : 'neg',\n",
    " 'gratitude' : 'pos',\n",
    " 'grief' : 'neg',\n",
    " 'joy' : 'pos',\n",
    " 'love' : 'pos',\n",
    " 'nervousness' : 'neg',\n",
    " 'optimism' : 'pos',\n",
    " 'pride' : 'pos',\n",
    " 'realization' : 'amb',\n",
    " 'relief' : 'pos',\n",
    " 'remorse' : 'neg',\n",
    " 'sadness' : 'neg',\n",
    " 'surprise' : 'amb',\n",
    " 'neutral' : 'amb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A class reresenting a mention of some entity. \n",
    "It consists of a text string, starting and ending indices\n",
    "of the text span and a paragraph and sentence id. \n",
    "'''\n",
    "class Mention:\n",
    "    def __init__(self, text, start, end, par_id, sent_id, global_sentence_id):\n",
    "        self.text =  text\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.par_id = par_id\n",
    "        self.sent_id = sent_id\n",
    "        self.global_sentence_id = global_sentence_id\n",
    "        self.POS = []\n",
    "        self.deps = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Mention in par \" + str(self.par_id) + \" Sentence \" \\\n",
    "              + str(self.sent_id) + \"(Global Sentence \"+self.global_sentence_id + \") text:pip install fuzzywuzzy[speedup]\\n\" + self.text\n",
    "        return rep\n",
    "\n",
    "'''\n",
    "A class reresenting a character. \n",
    "It consists of a list of mentions, a set of aliases, \n",
    "a list of verbs that the character is the actor of (agent),\n",
    "a list of verbs that the character is the receiver of (patient),\n",
    "a list of adjectives that the character is described with (description).  \n",
    "'''\n",
    "class Character:\n",
    "    def __init__(self, book, mainName):\n",
    "        self.book = book\n",
    "        self.mainName = mainName\n",
    "        self.mentions = []\n",
    "        self.unique_sents = {}\n",
    "        self.aliases = set()\n",
    "        self.agent = {}\n",
    "        self.patient = {}\n",
    "        self.description = {}\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Character: \" + self.mainName + \"\\n\"\n",
    "        return rep\n",
    "    \n",
    "    def to_json(self):\n",
    "        return {\"mainName\" : self.mainName,\n",
    "                \"mentions\" : [mention.text for mention in self.mentions],\n",
    "                \"aliases\" : list(self.aliases),\n",
    "                \"agent\" : [str(list(loc)) + \" \" + verb for loc, verb in self.agent.items()],\n",
    "                \"patient\" : [str(list(loc)) + \" \" + verb for loc, verb in self.patient.items()],\n",
    "        }\n",
    "    \n",
    "    '''\n",
    "    Add a mention of the character to a list of mentions.\n",
    "    '''\n",
    "    def update_mention(self, mention):\n",
    "        self.mentions.append(mention)\n",
    "        self.aliases.update([mention.text.lower()])\n",
    "        \n",
    "    '''\n",
    "    Match POS tags with character mentions\n",
    "    '''\n",
    "    def get_POS(self):\n",
    "        for mention in self.mentions:\n",
    "            span = range(mention.start, mention.end)\n",
    "            for loc in span:\n",
    "                mention.POS.append(self.book.pars[mention.par_id].POS_tags[loc])\n",
    "                mention.deps.update([mention.POS[-1]['dep']])\n",
    "                \n",
    "    def get_unique_sent_mentions(self):\n",
    "        for mention in self.mentions:\n",
    "            self.unique_sents[(mention.par_id, mention.sent_id, mention.global_sentence_id)] = \\\n",
    "            self.book.pars[mention.par_id].sents[mention.sent_id].text\n",
    "                \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the nsubj.\n",
    "    '''\n",
    "    def get_agent_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if 'nsubj' in mention.deps:\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['lemma']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.agent[global_loc] = verb\n",
    "                    \n",
    "                    \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the dobj or nsubjpass.\n",
    "    '''\n",
    "    def get_patient_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if ('dobj' in mention.deps) or ('nsubjpass' in mention.deps):\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['lemma']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.patient[global_loc] = verb\n",
    "                    \n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, book, par, text, sent_id, bound):\n",
    "        # Parent objects\n",
    "        self.book = book\n",
    "        self.par = par\n",
    "        # starting and ending boundary for the sentence (relative to the paragraph)\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Global starting and ending indices\n",
    "        self.globalStart = self.par.start + self.start\n",
    "        self.globalEnd = self.par.start + self.end \n",
    "        self.text = text\n",
    "        self.sent_id = sent_id\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par.par_id) \\\n",
    "        + \"\\nSentence \" + str(self.sent_id) + \" start \" + str(self.start) \\\n",
    "        + \" end \" + str(self.end) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "        \n",
    "'''\n",
    "A class for a paragraph.\n",
    "'''\n",
    "class Paragraph:\n",
    "    def __init__(self, book, text, par_id, bound):\n",
    "        # Starting and ending indices for the paragraph\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Refers to book parent object\n",
    "        self.book = book\n",
    "        # paragraph text\n",
    "        self.text = text\n",
    "        # paragraph index\n",
    "        self.par_id = par_id\n",
    "        # bool idnicating whether coref wroked\n",
    "        self.has_coref = False\n",
    "        # coref cluster list\n",
    "        self.coref_clusts = []\n",
    "        # list of sentences\n",
    "        self.sents = []\n",
    "        self.number_of_sentences = 0\n",
    "        self.sent_bounds = []\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par_id) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Split text into sentences\n",
    "    '''\n",
    "    def parse_into_sentences(self):\n",
    "        doc = nlp(self.text)\n",
    "        sentences = [sent for sent in doc.sents]\n",
    "        self.sents = [Sentence(self.book, self, sent.text, \n",
    "                               sent_id, (sent.start, sent.end)) \n",
    "                      for sent_id, sent in enumerate(sentences)]\n",
    "        self.number_of_sentences = len(sentences)\n",
    "        \n",
    "        \n",
    "    def run_coref_POS(self):\n",
    "        # Get coreference tags:\n",
    "        doc = nlp(self.text)\n",
    "        # Update whether the coref parser returned results\n",
    "        self.has_coref = doc._.has_coref\n",
    "        if doc._.has_coref:\n",
    "            self.coref_clusts = doc._.coref_clusters\n",
    "            self.book.parse_coref_clusts(self)\n",
    "        # Get POS tags:\n",
    "\n",
    "        # this uses the pretrained BERT model:\n",
    "        # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "        # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "        self.POS_tags = []\n",
    "        # Add POS tags to list for paragraph and for each individual sentence\n",
    "        for token_id in range(len(doc)):\n",
    "            token = doc[token_id]\n",
    "            sent_id = 0\n",
    "            for sent in self.sents:\n",
    "                if ((token_id >= sent.start) and (token_id <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "            self.sents[sent_id].POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "            self.POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "\n",
    "'''\n",
    "A class representing a book.\n",
    "'''        \n",
    "class Book:\n",
    "    def __init__(self, dataPath, fileName):\n",
    "        self.dataPath = dataPath\n",
    "        self.fileName = fileName\n",
    "        self.text = ''\n",
    "        self.characters = {}\n",
    "        self.char_mention_counts = collections.Counter()\n",
    "        self.top_characters = {}\n",
    "        self.pars = []\n",
    "        self.read_file()\n",
    "      \n",
    "    '''\n",
    "    A function for writing the book object to disk as a json file. Top indicates \n",
    "    whether we should just export top mentioned characters or all characters.\n",
    "    '''\n",
    "    def to_json(self, outFilename, top=False):\n",
    "        if not top:\n",
    "            representation = {\"fileName\" : self.fileName,\n",
    "                   \"characters\" : [character.to_json() for characterName, \n",
    "                                   character in self.characters.items()]}\n",
    "        else:\n",
    "            representation = {\"fileName\" : self.fileName,\n",
    "                   \"characters\" : [character.to_json() for characterName, \n",
    "                                   character in self.top_characters.items()]} \n",
    "        with open(outFilename, \"w\") as outFile:\n",
    "            json.dump(representation, outFile, separators=(',', ':'), indent=4)\n",
    "    \n",
    "    '''\n",
    "    Read the text of the book from a txt file.\n",
    "    '''\n",
    "    def read_file(self):\n",
    "        with open(os.path.join(self.dataPath, self.fileName), \"r\") as txtFile:\n",
    "            self.text = txtFile.read()\n",
    "        \n",
    "    '''\n",
    "    Break the text into paragraphs.\n",
    "    '''\n",
    "    def parse_into_pars(self):\n",
    "        # split on newlines followed by space\n",
    "        pars = re.split('\\n\\s', self.text)   \n",
    "#         par_bounds = [0]\n",
    "#         par_bounds += [m.start(0) for m in re.finditer('\\n\\s', self.text)]\n",
    "#         par_bounds.append(len(self.text) - 1)\n",
    "        # Replace newline chars\n",
    "        pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "        # Remove empty pars\n",
    "        pars = [par for par in pars if len(par) > 0]\n",
    "        \n",
    "        #Preprocess \"paragraphs\" that are actually quotes or single lined text\n",
    "        final_pars = []\n",
    "        for p,paragraph in enumerate(pars):\n",
    "            \n",
    "            if paragraph.count(\".\")<5:\n",
    "                if p==0:\n",
    "                    final_pars.append(paragraph)\n",
    "                else:\n",
    "                    final_pars[-1] = final_pars[-1] + \" \" + paragraph\n",
    "            else:\n",
    "                final_pars.append(paragraph)\n",
    "                \n",
    "                \n",
    "        par_bounds = []\n",
    "        par_loc = 0\n",
    "        for par in final_pars:\n",
    "            par_bounds.append(par_loc)\n",
    "            par_loc += len(par)\n",
    "        par_bounds.append(par_loc)\n",
    "        \n",
    "        \n",
    "        # Convert each paragraph into a Paragraph\n",
    "        self.pars = [Paragraph(self, par, par_id, (par_bounds[par_id],\n",
    "                                                   par_bounds[par_id+1])) \n",
    "                     for par_id, par in enumerate(final_pars)]\n",
    "        \n",
    "    '''\n",
    "    Parse the coreference clusters returned from the parsing of a paragraph\n",
    "    '''\n",
    "    def parse_coref_clusts(self, par):\n",
    "        clustList = par.coref_clusts\n",
    "        # Iterate over the coreference clusters\n",
    "        for idx, cluster in enumerate(clustList):\n",
    "            # get the main cluster identity\n",
    "            mainSpan = cluster.main.text.lower()\n",
    "            # If a character object does not yet exist, create one\n",
    "            max_fuzz = 70\n",
    "            max_fuzz_char = ''\n",
    "            for ch in self.characters:\n",
    "                pr = fuzz.partial_ratio(mainSpan, ch)\n",
    "                if pr>max_fuzz:\n",
    "                    max_fuzz = pr\n",
    "                    max_fuzz_char = ch\n",
    "            if max_fuzz>70:\n",
    "                # It is already present\n",
    "                character = self.characters[max_fuzz_char]\n",
    "            else:\n",
    "                character = Character(self, mainSpan)\n",
    "                # Add it to the dict of characters\n",
    "                self.characters[mainSpan] = character\n",
    "                \n",
    "            for mention in cluster.mentions:\n",
    "                # figure out which sentence the mention belongs to\n",
    "                sent_id = 0\n",
    "                for sent in par.sents:\n",
    "                    if ((mention.start >= sent.start) and (mention.end <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "                # create a mention object and add it to the character object\n",
    "                global_sentence_id = par.sent_bounds[0] + sent_id\n",
    "                mention = Mention(mention.text, mention.start, mention.end, par.par_id, sent_id, global_sentence_id)\n",
    "                character.update_mention(mention)\n",
    "                self.char_mention_counts[character.mainName] += 1\n",
    "                \n",
    "    def get_top_characters(self, n=5):\n",
    "        # Get n most mentioned characters\n",
    "        self.top_characters = self.char_mention_counts.most_common()[:n]\n",
    "        self.top_characters = {character[0] : self.characters[character[0]]\n",
    "                               for character in self.top_characters}\n",
    "                \n",
    "    def get_global_sentence_bounds(self):\n",
    "        sentence_begin = 0\n",
    "        for par in self.pars:\n",
    "            sentence_end = sentence_begin + par.number_of_sentences - 1\n",
    "            par.sent_bounds = [sentence_begin, sentence_end]\n",
    "            \n",
    "            sentence_begin = sentence_end+1\n",
    "        \n",
    "    def parse_text(self):\n",
    "        self.parse_into_pars()\n",
    "        for par in self.pars:\n",
    "            par.parse_into_sentences()\n",
    "        self.get_global_sentence_bounds()\n",
    "        for par in self.pars:\n",
    "            par.run_coref_POS()\n",
    "        for characterName, character in self.characters.items():\n",
    "            character.get_POS()\n",
    "            character.get_agent_verbs()\n",
    "            character.get_patient_verbs()\n",
    "            character.get_unique_sent_mentions()\n",
    "        self.get_top_characters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(metaDataFile):\n",
    "    with open(metaDataFile,'r') as fob:\n",
    "        metaBooks = json.load(fob)\n",
    "    return metaBooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parsing 1185 books from folder ../../datasets/Gutenberg/txt/\n",
      "Outputing results to ../results/book_parses\n",
      "\n",
      "Parsed Lewis Carroll___Alice's Adventures in Wonderland time passed 54.1 secs\n",
      "Parsed Lewis Carroll___Through the Looking-Glass time passed 116.7 secs\n",
      "Parsed Herman Melville___Moby Dick time passed 573.0 secs\n",
      "Parsed Thomas Hardy___Far from the Madding Crowd time passed 835.8 secs\n",
      "Parsed Nathaniel Hawthorne___The Scarlet Letter time passed 987.4 secs\n",
      "Parsed Washington Irving___The Legend of Sleepy Hollow time passed 1007.1 secs\n",
      "Parsed Charles Dickens___A Christmas Carol time passed 1062.1 secs\n",
      "Parsed Edgar Rice Burroughs___A Princess of Mars time passed 1190.9 secs\n",
      "Parsed Edgar Rice Burroughs___The Gods of Mars time passed 1345.2 secs\n",
      "Parsed Edgar Rice Burroughs___Warlord of Mars time passed 1476.3 secs\n",
      "Parsed Edgar Rice Burroughs___Thuvia, Maid of Mars time passed 1582.2 secs\n",
      "Parsed Mark Twain___The Adventures of Tom Sawyer, Complete time passed 1700.9 secs\n",
      "Parsed Mark Twain___Adventures of Huckleberry Finn, Complete time passed 1897.6 secs\n",
      "Parsed Nathaniel Hawthorne___The House of the Seven Gables time passed 2107.8 secs\n",
      "Parsed Edgar Rice Burroughs___Tarzan of the Apes time passed 2323.1 secs\n",
      "Parsed Edgar Rice Burroughs___The Return of Tarzan time passed 2494.7 secs\n",
      "Parsed Edgar Rice Burroughs___The Beasts of Tarzan time passed 2645.9 secs\n",
      "Parsed Mark Twain___A Connecticut Yankee in King Arthur's Court, Complete time passed 2869.4 secs\n",
      "Parsed Edgar Rice Burroughs___The Son of Tarzan time passed 3048.8 secs\n",
      "Parsed Mark Twain___Tom Sawyer Abroad time passed 3105.6 secs\n",
      "Parsed Edgar Rice Burroughs___Tarzan and the Jewels of Opar time passed 3244.2 secs\n",
      "Parsed Mark Twain___Tom Sawyer, Detective time passed 3281.9 secs\n",
      "Parsed Edgar Rice Burroughs___The Monster Men time passed 3407.4 secs\n",
      "Parsed Charles Dickens___A Tale of Two Cities time passed 3710.5 secs\n",
      "Parsed Mark Twain___The Tragedy of Pudd'nhead Wilson time passed 3809.8 secs\n",
      "Parsed Jane Austen___Persuasion time passed 3959.9 secs\n",
      "Parsed Edgar Rice Burroughs___Jungle Tales of Tarzan time passed 4099.9 secs\n",
      "Parsed Thomas Hardy___Far from the Madding Crowd time passed 4366.6 secs\n",
      "Parsed Thomas Hardy___Tess of the d'Urbervilles time passed 4673.5 secs\n",
      "Parsed Mark Twain___A Tramp Abroad time passed 5010.0 secs\n",
      "Parsed Robert Louis Stevenson___Treasure Island time passed 5128.8 secs\n",
      "Parsed Jane Austen___Northanger Abbey time passed 5264.7 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bd303a1361a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetaDataFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../results/metadata-final.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../results/book_parses\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetaDataFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-ead6a11bc1cf>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(dataPath, metaDataFile, outDir, top, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Parse the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# write outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7f42fb8f5d22>\u001b[0m in \u001b[0;36mparse_text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_sentence_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mpar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_coref_POS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcharacterName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharacters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mcharacter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_POS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7f42fb8f5d22>\u001b[0m in \u001b[0;36mrun_coref_POS\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_coref_POS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Get coreference tags:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Update whether the coref parser returned results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_coref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_coref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mneuralcoref.pyx\u001b[0m in \u001b[0;36mneuralcoref.neuralcoref.NeuralCoref.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Callable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/neural/_classes/relu.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput__BO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Callable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/thinc/neural/_classes/affine.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput__BI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define Paths and fileNames:\n",
    "dataPath = \"../../datasets/Gutenberg/txt/\"\n",
    "metaDataFile = \"../results/metadata-final.json\"\n",
    "outDir = \"../results/book_parses\"\n",
    "process_data(dataPath, metaDataFile, outDir, top=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for running the parsing pipeline on an entire dataset.\n",
    "Inputs:\n",
    "    DataPath - path to dir with the book txt files\n",
    "    metaDataFile - json filename with metaData for the dataset\n",
    "    outDir - directory where outputs will be written\n",
    "    [top] - bool indicating whether to also output just top characters as a separate set of output file\n",
    "    [verbose] - bool indicating whether to print progress and timing\n",
    "Outputs\n",
    "'''\n",
    "def process_data(dataPath, metaDataFile, outDir, top=True, verbose=False):\n",
    "    metaBooks = get_metadata(metaDataFile)\n",
    "    \n",
    "    if verbose:\n",
    "        time_start = time.time()\n",
    "        print(\"Starting parsing \" + str(len(metaBooks)) + \" books from folder \" + dataPath)\n",
    "        print(\"Outputing results to \" + outDir + \"\\n\")\n",
    "    \n",
    "    if not os.path.exists(outDir):\n",
    "        os.mkdirs(outDir)\n",
    "        \n",
    "    if top:\n",
    "        outDirTop = outDir + \"_top_only\"\n",
    "        if not os.path.exists(outDirTop):\n",
    "            os.mkdirs(outDir)\n",
    "            \n",
    "    # Iterate over books and run pipeline on each\n",
    "    for bookEntry in metaBooks:\n",
    "        fileName = bookEntry['filename']\n",
    "        title = os.path.splitext(fileName)[0]\n",
    "        outFileName = os.path.join(outDir, f'{title}_parsed.json')\n",
    "        \n",
    "        # Parse the book\n",
    "        b = Book(dataPath, fileName)\n",
    "        b.parse_text()\n",
    "        \n",
    "        # write outputs\n",
    "        b.to_json(outFileName)\n",
    "        if top:\n",
    "            outFileNameTop = os.path.join(outDirTop, f'{title}_parsed_top.json')\n",
    "            b.to_json(outFileNameTop, top=True)\n",
    "        \n",
    "        # optionally report progress\n",
    "        if verbose:\n",
    "            currTimeElapsed = str(round(time.time() - time_start, 1))\n",
    "            print(\"Parsed \" + title + \" time passed \" + currTimeElapsed + \" secs\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths and fileNames:\n",
    "dataPath = \"../../datasets/Gutenberg/txt/\"\n",
    "metaDataFile = \"../results/metadata-final.json\"\n",
    "# fileName = \"Herman Melville___Bartleby, The Scrivener.txt\"\n",
    "# title = \"Herman Melville___Bartleby, The Scrivener\"\n",
    "# stopwordsFileName = \"StopWords/jockers.stopwords\"\n",
    "title = \"Charles Dickens___A Christmas Carol\"\n",
    "fileName = f\"{title}.txt\"\n",
    "outDir = \"../results/book_parses\"\n",
    "outDirTop = \"../results/book_parses_top_only\"\n",
    "outFileName = os.path.join(outDir, f'{title}_parsed.json')\n",
    "outFileNameTop = os.path.join(outDirTop, f'{title}_parsed_top.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Book(dataPath, fileName)\n",
    "b.parse_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_json(outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_json(outFileNameTop, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = nnf.softmax(logits, dim=1).data.numpy().squeeze()\n",
    "    emotion = emotions[np.argmax(probs)]\n",
    "    emo_prob = np.max(probs)\n",
    "    mini_emotion = reduced_emotions[emotion]\n",
    "    return  emotion, mini_emotion, emo_prob, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emo_df = pd.DataFrame()\n",
    "# Iterate over characters with most mentions\n",
    "for character in b.top_characters:\n",
    "    char_emotion = []\n",
    "    # Iterate over sentences in which character is mentioned\n",
    "    for loc, sent in b.top_characters[character].unique_sents.items():\n",
    "        # Get emotional scores for each sentence with a character mention\n",
    "        emotion, mini_emotion, emo_prob, probs = get_emotion(sent)\n",
    "        char_emotion.append([character, loc[0],  loc[1], loc[2], emotion, mini_emotion, emo_prob])\n",
    "    # Store emotion results in data frame\n",
    "    char_df = pd.DataFrame(char_emotion, columns =['character', 'paragraph_id','sent_id','global_sent_id', \\\n",
    "                                                  'emotion', 'mini_emotion','emo_prob'])\n",
    "    emo_df = pd.concat([emo_df,char_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df['glob_sent_ratio'] = emo_df['global_sent_id']/b.pars[-1].sent_bounds[1]\n",
    "emo_df['glob_sent_grp'] = (emo_df['glob_sent_ratio']  / 0.025).apply(int)\n",
    "emo_df.groupby('character').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = emo_df[emo_df['character'].isin(['scrooge'])]\n",
    "# ed = ed[~ed['mini_emotion'].isin(['neutral'])]\n",
    "# Initialize a grid of plots with an Axes for each walk\n",
    "grid = sns.FacetGrid(ed, row=\"character\",col=\"mini_emotion\",# palette=\"tab20c\",\n",
    "                     )#, \n",
    "#                     width=1.5)\n",
    "\n",
    "# Draw a line plot to show the trajectory of each random walk\n",
    "# grid.map(plt.plot, \"glob_sent_ratio\", \"emo_prob\", marker=\"o\")\n",
    "grid.map(sns.scatterplot, \"glob_sent_ratio\", \"emo_prob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged = ed.groupby(['character','glob_sent_grp','mini_emotion']).agg({'global_sent_id':'count'})\n",
    "ged = ged/ged.groupby(level=[0,1]).sum()\n",
    "ged = ged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gedd = ged.pivot(index=['character','glob_sent_grp'], columns='mini_emotion', values='global_sent_id').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"glob_sent_grp\", y=\"global_sent_id\",\n",
    "             hue=\"mini_emotion\",\n",
    "             data=ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gedd['glob_sent_grp']/(len(gedd)+1),gedd['pos']-gedd['neg'], 'b-')\n",
    "plt.title('Net Emotion Proportion Vs Narrative Time for Scrooge')\n",
    "plt.xlabel('Narrative Time')\n",
    "plt.ylabel('Net Emotion Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing polynomial fit\n",
    "# x = gedd['glob_sent_grp']/(len(gedd)+1)\n",
    "# y = gedd['pos']-gedd['neg']\n",
    "# deg = 10\n",
    "# poly = np.polyfit(x, y, deg)\n",
    "# yhat = np.zeros_like(x)\n",
    "# for deg, coeff in enumerate(poly):\n",
    "#     yhat += coeff * x**(len(poly) - 1 - deg)\n",
    "# plt.plot(x, y, 'b.')\n",
    "# plt.plot(x, yhat, 'r-')\n",
    "# plt.title('Net Emotion Proportion Vs Narrative Time for Scrooge with Poly Fit')\n",
    "# plt.xlabel('Narrative Time')\n",
    "# plt.ylabel('Net Emotion Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Split text into sentences\n",
    "# '''\n",
    "# def parse_into_sentences(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     sentences = re.split('\\.|!|\\?', doc)   \n",
    "#     sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "#     return sentences\n",
    "# '''\n",
    "# Split text into paragraphs\n",
    "# '''\n",
    "# def parse_into_pars(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     pars = re.split('\\n\\s', doc)   \n",
    "#     pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "#     return pars\n",
    "# '''\n",
    "# Create list of stop words (currently not used)\n",
    "# '''\n",
    "# def read_stopwords(filename):\n",
    "#     stopwords={}\n",
    "#     with open(filename) as file:\n",
    "#         for line in file:\n",
    "#             stopwords[line.rstrip()]=1\n",
    "#     return stopwords\n",
    "\n",
    "# '''\n",
    "# Clean sentences by removing trailing punctuation on words, \n",
    "# and converting to lowercase\n",
    "# '''\n",
    "# def clean_sentences(sentences, charsTOStrip = '\\\"\\', '):\n",
    "#     texts = [\n",
    "#         [word.strip(charsTOStrip) for word in sentence.lower().split()]\n",
    "#         for sentence in sentences]\n",
    "#     return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Convert list of neuralcoref.neuralcoref.Cluster objects \n",
    "# to list of dicts so that we can serialize it. \n",
    "# '''\n",
    "# def convert_clust_to_list_dict(clustList):\n",
    "#     dictList = []\n",
    "#     characters = {character : {\n",
    "#     \"aliases\" : set(),\n",
    "#     \"agent\" : [],\n",
    "#     \"patient\" : [],\n",
    "#     \"description\" : [],\n",
    "#     } for character in top_characters}\n",
    "    \n",
    "#     for idx, cluster in enumerate(clustList):\n",
    "#         mainSpan = cluster.main\n",
    "#         dictList.append({mainSpan.text.lower() : [{'start' : mention.start, 'end': mention.end, 'text' : mention.text}\n",
    "#                                            for mention in cluster.mentions]})\n",
    "#         characters[mainSpan.text.lower()] += 1\n",
    "#     return dictList, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating book data structure that we will use.\n",
    "# # For now it is just a dict, but maybe we can make it a class\n",
    "# def create_book_struct(fileName):\n",
    "    \n",
    "#     characters = collections.Counter()\n",
    "    \n",
    "#     time_start = time.time()\n",
    "\n",
    "#     # break file into paragraphs\n",
    "#     pars = parse_into_pars(fileName)\n",
    "#     #parse_into_sentences(fileName)\n",
    "\n",
    "#     # initialize book obj\n",
    "#     book = {}\n",
    "#     # Iterate over each sentence and \n",
    "#     for idx, sentence in enumerate(sentences):\n",
    "\n",
    "#         # Get sentence id and text\n",
    "#         book[idx] = {}\n",
    "#         book[idx]['text'] = sentence\n",
    "\n",
    "#         # Get coreference tags:\n",
    "#         doc = nlp(sentence)\n",
    "#         book[idx]['has_coref'] = doc._.has_coref\n",
    "#         if doc._.has_coref:\n",
    "#             coref_clusts, chars = convert_clust_to_list_dict(doc._.coref_clusters)\n",
    "#             book[idx]['coref_clusts'] = coref_clusts\n",
    "            \n",
    "#             characters += chars\n",
    "#         else:\n",
    "#             book[idx]['coref_clusts'] = []\n",
    "\n",
    "#         # Get POS tags:\n",
    "\n",
    "#         # this uses the pretrained BERT model:\n",
    "#         # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "#         # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "#         book[idx][\"POS_tags\"] = []\n",
    "#         for token in doc:\n",
    "#             book[idx][\"POS_tags\"].append({\n",
    "#                 'text' : token.text,\n",
    "#                 'lemma' : token.lemma_,\n",
    "#                 'pos' : token.pos_, \n",
    "#                 'tag' : token.tag_, \n",
    "#                 'dep' : token.dep_})\n",
    "#     time_end = time.time()\n",
    "#     print(\"Parsing book took \", round(time_end - time_start, 2), \" secs\")\n",
    "#     return book, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book, characters = create_book_struct(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_book_to_disk(book, outFileName):\n",
    "#     with open (os.path.join(dataPath, outFileName), 'w') as outFile:\n",
    "#         json.dump(book, outFile, separators=(',', ':'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_book_to_disk(book, outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only get top n most mentioned characters. \n",
    "# # Probably we should replace this with some kind of \n",
    "# # filter based on the min number of mentions\n",
    "# most_common_n = 20\n",
    "# top_characters = characters.most_common()[:most_common_n]\n",
    "# top_characters = [top_character[0] for top_character in top_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('''\n",
    "# I am a rather elderly man.  The nature of my avocations for the last\n",
    "# thirty years has brought me into more than ordinary contact with what\n",
    "# would seem an interesting and somewhat singular set of men, of whom as\n",
    "# yet nothing that I know of has ever been written:--I mean the\n",
    "# law-copyists or scriveners.  I have known very many of them,\n",
    "# professionally and privately, and if I pleased, could relate divers\n",
    "# histories, at which good-natured gentlemen might smile, and sentimental\n",
    "# souls might weep.  But I waive the biographies of all other scriveners\n",
    "# for a few passages in the life of Bartleby, who was a scrivener of the\n",
    "# strangest I ever saw or heard of.  While of other law-copyists I might\n",
    "# write the complete life, of Bartleby nothing of that sort can be done.\n",
    "# I believe that no materials exist for a full and satisfactory biography\n",
    "# of this man.  It is an irreparable loss to literature.  Bartleby was one\n",
    "# of those beings of whom nothing is ascertainable, except from the\n",
    "# original sources, and in his case those are very small.  What my own\n",
    "# astonished eyes saw of Bartleby, _that_ is all I know of him, except,\n",
    "# indeed, one vague report which will appear in the sequel.\n",
    "# ''')\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
