{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nuwandavek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/nuwandavek/anaconda3/envs/peace/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.1.0) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f6528742880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp, blacklist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\", return_dict=True)\n",
    "import numpy as np\n",
    "emotions = ['admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity',\\\n",
    "            'desire','disappointment','disapproval','disgust','embarrassment','excitement','fear',\\\n",
    "            'gratitude','grief','joy','love','nervousness','optimism','pride','realization','relief',\\\n",
    "            'remorse','sadness','surprise','neutral']\n",
    "reduced_emotions = {\n",
    "    'admiration' : 'pos',\n",
    " 'amusement' : 'pos',\n",
    " 'anger' : 'neg',\n",
    " 'annoyance' : 'neg',\n",
    " 'approval' : 'pos',\n",
    " 'caring' : 'pos',\n",
    " 'confusion' : 'amb',\n",
    " 'curiosity' : 'amb',\n",
    " 'desire' : 'pos',\n",
    " 'disappointment' : 'neg',\n",
    " 'disapproval' : 'neg',\n",
    " 'disgust' : 'neg',\n",
    " 'embarrassment' : 'neg',\n",
    " 'excitement' : 'pos',\n",
    " 'fear' : 'neg',\n",
    " 'gratitude' : 'pos',\n",
    " 'grief' : 'neg',\n",
    " 'joy' : 'pos',\n",
    " 'love' : 'pos',\n",
    " 'nervousness' : 'neg',\n",
    " 'optimism' : 'pos',\n",
    " 'pride' : 'pos',\n",
    " 'realization' : 'amb',\n",
    " 'relief' : 'pos',\n",
    " 'remorse' : 'neg',\n",
    " 'sadness' : 'neg',\n",
    " 'surprise' : 'amb',\n",
    " 'neutral' : 'amb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A class reresenting a mention of some entity. \n",
    "It consists of a text string, starting and ending indices\n",
    "of the text span and a paragraph and sentence id. \n",
    "'''\n",
    "class Mention:\n",
    "    def __init__(self, text, start, end, par_id, sent_id, global_sentence_id):\n",
    "        self.text =  text\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.par_id = par_id\n",
    "        self.sent_id = sent_id\n",
    "        self.global_sentence_id = global_sentence_id\n",
    "        self.POS = []\n",
    "        self.deps = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Mention in par \" + str(self.par_id) + \" Sentence \" \\\n",
    "              + str(self.sent_id) + \"(Global Sentence \"+self.global_sentence_id + \") text:pip install fuzzywuzzy[speedup]\\n\" + self.text\n",
    "        return rep\n",
    "\n",
    "'''\n",
    "A class reresenting a character. \n",
    "It consists of a list of mentions, a set of aliases, \n",
    "a list of verbs that the character is the actor of (agent),\n",
    "a list of verbs that the character is the receiver of (patient),\n",
    "a list of adjectives that the character is described with (description).  \n",
    "'''\n",
    "class Character:\n",
    "    def __init__(self, book, mainName):\n",
    "        self.book = book\n",
    "        self.mainName = mainName\n",
    "        self.mentions = []\n",
    "        self.unique_sents = {}\n",
    "        self.aliases = set()\n",
    "        self.agent = {}\n",
    "        self.patient = {}\n",
    "        self.description = {}\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Character: \" + self.mainName + \"\\n\"\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Add a mention of the character to a list of mentions.\n",
    "    '''\n",
    "    def update_mention(self, mention):\n",
    "        self.mentions.append(mention)\n",
    "        self.aliases.update([mention.text.lower()])\n",
    "        \n",
    "    '''\n",
    "    Match POS tags with character mentions\n",
    "    '''\n",
    "    def get_POS(self):\n",
    "        for mention in self.mentions:\n",
    "            span = range(mention.start, mention.end)\n",
    "            for loc in span:\n",
    "                mention.POS.append(self.book.pars[mention.par_id].POS_tags[loc])\n",
    "                mention.deps.update([mention.POS[-1]['dep']])\n",
    "                \n",
    "    def get_unique_sent_mentions(self):\n",
    "        for mention in self.mentions:\n",
    "            self.unique_sents[(mention.par_id, mention.sent_id, mention.global_sentence_id)] = \\\n",
    "            self.book.pars[mention.par_id].sents[mention.sent_id].text\n",
    "                \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the nsubj.\n",
    "    '''\n",
    "    def get_agent_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if 'nsubj' in mention.deps:\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['lemma']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.agent[global_loc] = verb\n",
    "                    \n",
    "                    \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the dobj or nsubjpass.\n",
    "    '''\n",
    "    def get_patient_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if ('dobj' in mention.deps) or ('nsubjpass' in mention.deps):\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['lemma']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.patient[global_loc] = verb\n",
    "                    \n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, book, par, text, sent_id, bound):\n",
    "        # Parent objects\n",
    "        self.book = book\n",
    "        self.par = par\n",
    "        # starting and ending boundary for the sentence (relative to the paragraph)\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Global starting and ending indices\n",
    "        self.globalStart = self.par.start + self.start\n",
    "        self.globalEnd = self.par.start + self.end \n",
    "        self.text = text\n",
    "        self.sent_id = sent_id\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par.par_id) \\\n",
    "        + \"\\nSentence \" + str(self.sent_id) + \" start \" + str(self.start) \\\n",
    "        + \" end \" + str(self.end) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "        \n",
    "'''\n",
    "A class for a paragraph.\n",
    "'''\n",
    "class Paragraph:\n",
    "    def __init__(self, book, text, par_id, bound):\n",
    "        # Starting and ending indices for the paragraph\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Refers to book parent object\n",
    "        self.book = book\n",
    "        # paragraph text\n",
    "        self.text = text\n",
    "        # paragraph index\n",
    "        self.par_id = par_id\n",
    "        # bool idnicating whether coref wroked\n",
    "        self.has_coref = False\n",
    "        # coref cluster list\n",
    "        self.coref_clusts = []\n",
    "        # list of sentences\n",
    "        self.sents = []\n",
    "        self.number_of_sentences = 0\n",
    "        self.sent_bounds = []\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par_id) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Split text into sentences\n",
    "    '''\n",
    "    def parse_into_sentences(self):\n",
    "        doc = nlp(self.text)\n",
    "        sentences = [sent for sent in doc.sents]\n",
    "        self.sents = [Sentence(self.book, self, sent.text, \n",
    "                               sent_id, (sent.start, sent.end)) \n",
    "                      for sent_id, sent in enumerate(sentences)]\n",
    "        self.number_of_sentences = len(sentences)\n",
    "        \n",
    "        \n",
    "    def run_coref_POS(self):\n",
    "        # Get coreference tags:\n",
    "        doc = nlp(self.text)\n",
    "        # Update whether the coref parser returned results\n",
    "        self.has_coref = doc._.has_coref\n",
    "        if doc._.has_coref:\n",
    "            self.coref_clusts = doc._.coref_clusters\n",
    "            self.book.parse_coref_clusts(self)\n",
    "        # Get POS tags:\n",
    "\n",
    "        # this uses the pretrained BERT model:\n",
    "        # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "        # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "        self.POS_tags = []\n",
    "        # Add POS tags to list for paragraph and for each individual sentence\n",
    "        for token_id in range(len(doc)):\n",
    "            token = doc[token_id]\n",
    "            sent_id = 0\n",
    "            for sent in self.sents:\n",
    "                if ((token_id >= sent.start) and (token_id <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "            self.sents[sent_id].POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "            self.POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "\n",
    "'''\n",
    "A class representing a book.\n",
    "'''        \n",
    "class Book:\n",
    "    def __init__(self, dataPath, fileName):\n",
    "        self.dataPath = dataPath\n",
    "        self.fileName = fileName\n",
    "        self.text = ''\n",
    "        self.characters = {}\n",
    "        self.char_mention_counts = collections.Counter()\n",
    "        self.top_characters = {}\n",
    "        self.pars = []\n",
    "        self.read_file()\n",
    "    \n",
    "    '''\n",
    "    Read the text of the book from a txt file.\n",
    "    '''\n",
    "    def read_file(self):\n",
    "        with open(os.path.join(self.dataPath, self.fileName), \"r\") as txtFile:\n",
    "            self.text = txtFile.read()\n",
    "        \n",
    "    '''\n",
    "    Break the text into paragraphs.\n",
    "    '''\n",
    "    def parse_into_pars(self):\n",
    "        # split on newlines followed by space\n",
    "        pars = re.split('\\n\\s', self.text)   \n",
    "#         par_bounds = [0]\n",
    "#         par_bounds += [m.start(0) for m in re.finditer('\\n\\s', self.text)]\n",
    "#         par_bounds.append(len(self.text) - 1)\n",
    "        # Replace newline chars\n",
    "        pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "        # Remove empty pars\n",
    "        pars = [par for par in pars if len(par) > 0]\n",
    "        \n",
    "        #Preprocess \"paragraphs\" that are actually quotes or single lined text\n",
    "        final_pars = []\n",
    "        for p,paragraph in enumerate(pars):\n",
    "            \n",
    "            if paragraph.count(\".\")<5:\n",
    "                if p==0:\n",
    "                    final_pars.append(paragraph)\n",
    "                else:\n",
    "                    final_pars[-1] = final_pars[-1] + \" \" + paragraph\n",
    "            else:\n",
    "                final_pars.append(paragraph)\n",
    "                \n",
    "                \n",
    "        par_bounds = []\n",
    "        par_loc = 0\n",
    "        for par in final_pars:\n",
    "            par_bounds.append(par_loc)\n",
    "            par_loc += len(par)\n",
    "        par_bounds.append(par_loc)\n",
    "        \n",
    "        \n",
    "        # Convert each paragraph into a Paragraph\n",
    "        self.pars = [Paragraph(self, par, par_id, (par_bounds[par_id],\n",
    "                                                   par_bounds[par_id+1])) \n",
    "                     for par_id, par in enumerate(final_pars)]\n",
    "        \n",
    "    '''\n",
    "    Parse the coreference clusters returned from the parsing of a paragraph\n",
    "    '''\n",
    "    def parse_coref_clusts(self, par):\n",
    "        clustList = par.coref_clusts\n",
    "        # Iterate over the coreference clusters\n",
    "        for idx, cluster in enumerate(clustList):\n",
    "            # get the main cluster identity\n",
    "            mainSpan = cluster.main.text.lower()\n",
    "            # If a character object does not yet exist, create one\n",
    "            max_fuzz = 70\n",
    "            max_fuzz_char = ''\n",
    "            for ch in self.characters:\n",
    "                pr = fuzz.partial_ratio(mainSpan, ch)\n",
    "                if pr>max_fuzz:\n",
    "                    max_fuzz = pr\n",
    "                    max_fuzz_char = ch\n",
    "            if max_fuzz>70:\n",
    "                # It is already present\n",
    "                character = self.characters[max_fuzz_char]\n",
    "            else:\n",
    "                character = Character(self, mainSpan)\n",
    "                # Add it to the dict of characters\n",
    "                self.characters[mainSpan] = character\n",
    "                \n",
    "            for mention in cluster.mentions:\n",
    "                # figure out which sentence the mention belongs to\n",
    "                sent_id = 0\n",
    "                for sent in par.sents:\n",
    "                    if ((mention.start >= sent.start) and (mention.end <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "                # create a mention object and add it to the character object\n",
    "                global_sentence_id = par.sent_bounds[0] + sent_id\n",
    "                mention = Mention(mention.text, mention.start, mention.end, par.par_id, sent_id, global_sentence_id)\n",
    "                character.update_mention(mention)\n",
    "                self.char_mention_counts[character.mainName] += 1\n",
    "                \n",
    "    def get_top_characters(self, n=5):\n",
    "        # Get n most mentioned characters\n",
    "        self.top_characters = self.char_mention_counts.most_common()[:n]\n",
    "        self.top_characters = {character[0] : self.characters[character[0]]\n",
    "                               for character in self.top_characters}\n",
    "                \n",
    "    def get_global_sentence_bounds(self):\n",
    "        sentence_begin = 0\n",
    "        for par in self.pars:\n",
    "            sentence_end = sentence_begin + par.number_of_sentences - 1\n",
    "            par.sent_bounds = [sentence_begin, sentence_end]\n",
    "            \n",
    "            sentence_begin = sentence_end+1\n",
    "        \n",
    "    def parse_text(self):\n",
    "        self.parse_into_pars()\n",
    "        for par in self.pars:\n",
    "            par.parse_into_sentences()\n",
    "        self.get_global_sentence_bounds()\n",
    "        for par in self.pars:\n",
    "            par.run_coref_POS()\n",
    "        for characterName, character in self.characters.items():\n",
    "            character.get_POS()\n",
    "            character.get_agent_verbs()\n",
    "            character.get_patient_verbs()\n",
    "            character.get_unique_sent_mentions()\n",
    "        self.get_top_characters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths and fileNames:\n",
    "dataPath = \"../../datasets/Gutenberg/txt/\"\n",
    "# fileName = \"Herman Melville___Bartleby, The Scrivener.txt\"\n",
    "# title = \"Herman Melville___Bartleby, The Scrivener\"\n",
    "title = \"Charles Dickens___A Christmas Carol\"\n",
    "fileName = f\"{title}.txt\"\n",
    "stopwordsFileName = \"StopWords/jockers.stopwords\"\n",
    "outFileName = f'{title}_sentences.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Book(dataPath, fileName)\n",
    "b.parse_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = nnf.softmax(logits, dim=1).data.numpy().squeeze()\n",
    "    emotion = emotions[np.argmax(probs)]\n",
    "    emo_prob = np.max(probs)\n",
    "    mini_emotion = reduced_emotions[emotion]\n",
    "    return  emotion, mini_emotion, emo_prob, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4dd017c0d891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_characters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Get emotional scores for each sentence with a character mention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mchar_emotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcharacter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_emotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_prob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Store emotion results in data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c9320a88233f>\u001b[0m in \u001b[0;36mget_emotion\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         )\n\u001b[0;32m--> 833\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 )\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    477\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         )\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emo_df = pd.DataFrame()\n",
    "# Iterate over characters with most mentions\n",
    "for character in b.top_characters:\n",
    "    char_emotion = []\n",
    "    # Iterate over sentences in which character is mentioned\n",
    "    for loc, sent in b.top_characters[character].unique_sents.items():\n",
    "        # Get emotional scores for each sentence with a character mention\n",
    "        emotion, mini_emotion, emo_prob, probs = get_emotion(sent)\n",
    "        char_emotion.append([character, loc[0],  loc[1], loc[2], emotion, mini_emotion, emo_prob])\n",
    "    # Store emotion results in data frame\n",
    "    char_df = pd.DataFrame(char_emotion, columns =['character', 'paragraph_id','sent_id','global_sent_id', \\\n",
    "                                                  'emotion', 'mini_emotion','emo_prob'])\n",
    "    emo_df = pd.concat([emo_df,char_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataPath + fileName) as fob:\n",
    "    txt = fob.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n",
      "183\n",
      "[96, 101, 82, 167, 181, 508, 57, 78, 82, 103, 51, 116, 226, 152, 805, 130, 41, 24, 62, 223, 94, 64, 275, 227, 115, 171, 142, 240, 61, 114, 46, 128, 169, 97, 150, 206, 150, 105, 536, 153, 188, 216, 178, 67, 128, 150, 101, 53, 104, 393, 81, 210, 354, 120, 226, 217, 140, 199, 164, 138, 44, 177, 169, 221, 267, 28, 108, 80, 301, 138, 136, 79, 157, 109, 58, 125, 87, 73, 50, 66, 94, 236, 324, 313, 18, 298, 243, 244, 257, 214, 157, 164, 540, 625, 518, 181, 200, 170, 82, 90, 425, 13, 129, 60, 232, 89, 414, 359, 289, 96, 22, 124, 67, 234, 189, 172, 315, 226, 209, 190, 89, 232, 133, 808, 97, 86, 23, 87, 20, 232, 51, 77, 130, 140, 181, 58, 113, 82, 109, 33, 233, 79, 163, 136, 234, 113, 140, 288, 132, 115, 148, 68, 50, 49, 28, 72, 120, 412, 63, 66, 51, 279, 56, 189, 76, 176, 163, 139, 81, 88, 200, 197, 54, 75, 15, 135, 106, 15, 95, 116, 15, 215, 211]\n"
     ]
    }
   ],
   "source": [
    "pars = re.split('\\n\\s', txt)   \n",
    "pars = [par.replace(\"\\n\", \" \").strip() for par in pars]\n",
    "pars = [par for par in pars if len(par) > 0]\n",
    "print(len(pars))\n",
    "\n",
    "final_pars = {'0' : {\n",
    "    'sum' : 0,\n",
    "    'idxs' : []\n",
    "}}\n",
    "cur_par = 0\n",
    "\n",
    "for p,paragraph in enumerate(pars):\n",
    "    if \n",
    "    \n",
    "    \n",
    "    \n",
    "print([x.count(\" \") for x in final_pars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The door of Scrooge\\'s counting-house was open, that he might keep his eye upon his clerk, who in a dismal little cell beyond, a sort of tank, was copying letters. Scrooge had a very small fire, but the clerk\\'s fire was so very much smaller that it looked like one coal. But he couldn\\'t replenish it, for Scrooge kept the coal-box in his own room; and so surely as the clerk came in with the shovel, the master predicted that it would be necessary for them to part. Wherefore the clerk put on his white comforter, and tried to warm himself at the candle; in which effort, not being a man of strong imagination, he failed. \"A merry Christmas, uncle! God save you!\" cried a cheerful voice. It was the voice of Scrooge\\'s nephew, who came upon him so quickly that this was the first intimation he had of his approach. \"Bah!\" said Scrooge. \"Humbug!\" He had so heated himself with rapid walking in the fog and frost, this nephew of Scrooge\\'s, that he was all in a glow; his face was ruddy and handsome; his eyes sparkled, and his breath smoked again. \"Christmas a humbug, uncle!\" said Scrooge\\'s nephew. \"You don\\'t mean that, I am sure?\" \"I do,\" said Scrooge. \"Merry Christmas! What right have you to be merry? What reason have you to be merry? You\\'re poor enough.\" \"Come, then,\" returned the nephew gaily. \"What right have you to be dismal? What reason have you to be morose? You\\'re rich enough.\" Scrooge, having no better answer ready on the spur of the moment, said, \"Bah!\" again; and followed it up with \"Humbug!\" \"Don\\'t be cross, uncle!\" said the nephew. [Illustration: _\"A Merry Christmas, uncle! God save you!\" cried a cheerful voice._] \"What else can I be,\" returned the uncle, \"when I live in such a world of fools as this? Merry Christmas! Out upon merry Christmas! What\\'s Christmas-time to you but a time for paying bills without money; a time for finding yourself a year older, and not an hour richer; a time for balancing your books, and having every item in \\'em through a round dozen of months presented dead against you? If I could work my will,\" said Scrooge indignantly, \"every idiot who goes about with \\'Merry Christmas\\' on his lips should be boiled with his own pudding, and buried with a stake of holly through his heart. He should!\" \"Uncle!\" pleaded the nephew. \"Nephew!\" returned the uncle sternly, \"keep Christmas in your own way, and let me keep it in mine.\" \"Keep it!\" repeated Scrooge\\'s nephew. \"But you don\\'t keep it.\" \"Let me leave it alone, then,\" said Scrooge. \"Much good may it do you! Much good it has ever done you!\" \"There are many things from which I might have derived good, by which I have not profited, I dare say,\" returned the nephew; \"Christmas among the rest. But I am sure I have always thought of Christmas-time, when it has come round--apart from the veneration due to its sacred name and origin, if anything belonging to it can be apart from that--as a good time; a kind, forgiving, charitable, pleasant time; the only time I know of, in the long calendar of the year, when men and women seem by one consent to open their shut-up hearts freely, and to think of people below them as if they really were fellow-passengers to the grave, and not another race of creatures bound on other journeys. And therefore, uncle, though it has never put a scrap of gold or silver in my pocket, I believe that it _has_ done me good, and _will_ do me good; and I say, God bless it!\" The clerk in the tank involuntarily applauded. Becoming immediately sensible of the impropriety, he poked the fire, and extinguished the last frail spark for ever. \"Let me hear another sound from _you_,\" said Scrooge, \"and you\\'ll keep your Christmas by losing your situation! You\\'re quite a powerful speaker, sir,\" he added, turning to his nephew. \"I wonder you don\\'t go into Parliament.\" \"Don\\'t be angry, uncle. Come! Dine with us to-morrow.\" Scrooge said that he would see him----Yes, indeed he did. He went the whole length of the expression, and said that he would see him in that extremity first. \"But why?\" cried Scrooge\\'s nephew. \"Why?\" \"Why did you get married?\" said Scrooge. \"Because I fell in love.\" \"Because you fell in love!\" growled Scrooge, as if that were the only one thing in the world more ridiculous than a merry Christmas. \"Good afternoon!\" \"Nay, uncle, but you never came to see me before that happened. Why give it as a reason for not coming now?\" \"Good afternoon,\" said Scrooge. \"I want nothing from you; I ask nothing of you; why cannot we be friends?\" \"Good afternoon!\" said Scrooge.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pars[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df['glob_sent_ratio'] = emo_df['global_sent_id']/b.pars[-1].sent_bounds[1]\n",
    "emo_df['glob_sent_grp'] = (emo_df['glob_sent_ratio']  / 0.025).apply(int)\n",
    "emo_df.groupby('character').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = emo_df[emo_df['character'].isin(['scrooge'])]\n",
    "# ed = ed[~ed['mini_emotion'].isin(['neutral'])]\n",
    "# Initialize a grid of plots with an Axes for each walk\n",
    "grid = sns.FacetGrid(ed, row=\"character\",col=\"mini_emotion\",# palette=\"tab20c\",\n",
    "                     )#, \n",
    "#                     width=1.5)\n",
    "\n",
    "# Draw a line plot to show the trajectory of each random walk\n",
    "# grid.map(plt.plot, \"glob_sent_ratio\", \"emo_prob\", marker=\"o\")\n",
    "grid.map(sns.scatterplot, \"glob_sent_ratio\", \"emo_prob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged = ed.groupby(['character','glob_sent_grp','mini_emotion']).agg({'global_sent_id':'count'})\n",
    "ged = ged/ged.groupby(level=[0,1]).sum()\n",
    "ged = ged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gedd = ged.pivot(index=['character','glob_sent_grp'], columns='mini_emotion', values='global_sent_id').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"glob_sent_grp\", y=\"global_sent_id\",\n",
    "             hue=\"mini_emotion\",\n",
    "             data=ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gedd['glob_sent_grp']/(len(gedd)+1),gedd['pos']-gedd['neg'], 'b-')\n",
    "plt.title('Net Emotion Proportion Vs Narrative Time for Scrooge')\n",
    "plt.xlabel('Narrative Time')\n",
    "plt.ylabel('Net Emotion Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing polynomial fit\n",
    "# x = gedd['glob_sent_grp']/(len(gedd)+1)\n",
    "# y = gedd['pos']-gedd['neg']\n",
    "# deg = 10\n",
    "# poly = np.polyfit(x, y, deg)\n",
    "# yhat = np.zeros_like(x)\n",
    "# for deg, coeff in enumerate(poly):\n",
    "#     yhat += coeff * x**(len(poly) - 1 - deg)\n",
    "# plt.plot(x, y, 'b.')\n",
    "# plt.plot(x, yhat, 'r-')\n",
    "# plt.title('Net Emotion Proportion Vs Narrative Time for Scrooge with Poly Fit')\n",
    "# plt.xlabel('Narrative Time')\n",
    "# plt.ylabel('Net Emotion Proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Split text into sentences\n",
    "# '''\n",
    "# def parse_into_sentences(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     sentences = re.split('\\.|!|\\?', doc)   \n",
    "#     sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "#     return sentences\n",
    "# '''\n",
    "# Split text into paragraphs\n",
    "# '''\n",
    "# def parse_into_pars(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     pars = re.split('\\n\\s', doc)   \n",
    "#     pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "#     return pars\n",
    "# '''\n",
    "# Create list of stop words (currently not used)\n",
    "# '''\n",
    "# def read_stopwords(filename):\n",
    "#     stopwords={}\n",
    "#     with open(filename) as file:\n",
    "#         for line in file:\n",
    "#             stopwords[line.rstrip()]=1\n",
    "#     return stopwords\n",
    "\n",
    "# '''\n",
    "# Clean sentences by removing trailing punctuation on words, \n",
    "# and converting to lowercase\n",
    "# '''\n",
    "# def clean_sentences(sentences, charsTOStrip = '\\\"\\', '):\n",
    "#     texts = [\n",
    "#         [word.strip(charsTOStrip) for word in sentence.lower().split()]\n",
    "#         for sentence in sentences]\n",
    "#     return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Convert list of neuralcoref.neuralcoref.Cluster objects \n",
    "# to list of dicts so that we can serialize it. \n",
    "# '''\n",
    "# def convert_clust_to_list_dict(clustList):\n",
    "#     dictList = []\n",
    "#     characters = {character : {\n",
    "#     \"aliases\" : set(),\n",
    "#     \"agent\" : [],\n",
    "#     \"patient\" : [],\n",
    "#     \"description\" : [],\n",
    "#     } for character in top_characters}\n",
    "    \n",
    "#     for idx, cluster in enumerate(clustList):\n",
    "#         mainSpan = cluster.main\n",
    "#         dictList.append({mainSpan.text.lower() : [{'start' : mention.start, 'end': mention.end, 'text' : mention.text}\n",
    "#                                            for mention in cluster.mentions]})\n",
    "#         characters[mainSpan.text.lower()] += 1\n",
    "#     return dictList, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating book data structure that we will use.\n",
    "# # For now it is just a dict, but maybe we can make it a class\n",
    "# def create_book_struct(fileName):\n",
    "    \n",
    "#     characters = collections.Counter()\n",
    "    \n",
    "#     time_start = time.time()\n",
    "\n",
    "#     # break file into paragraphs\n",
    "#     pars = parse_into_pars(fileName)\n",
    "#     #parse_into_sentences(fileName)\n",
    "\n",
    "#     # initialize book obj\n",
    "#     book = {}\n",
    "#     # Iterate over each sentence and \n",
    "#     for idx, sentence in enumerate(sentences):\n",
    "\n",
    "#         # Get sentence id and text\n",
    "#         book[idx] = {}\n",
    "#         book[idx]['text'] = sentence\n",
    "\n",
    "#         # Get coreference tags:\n",
    "#         doc = nlp(sentence)\n",
    "#         book[idx]['has_coref'] = doc._.has_coref\n",
    "#         if doc._.has_coref:\n",
    "#             coref_clusts, chars = convert_clust_to_list_dict(doc._.coref_clusters)\n",
    "#             book[idx]['coref_clusts'] = coref_clusts\n",
    "            \n",
    "#             characters += chars\n",
    "#         else:\n",
    "#             book[idx]['coref_clusts'] = []\n",
    "\n",
    "#         # Get POS tags:\n",
    "\n",
    "#         # this uses the pretrained BERT model:\n",
    "#         # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "#         # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "#         book[idx][\"POS_tags\"] = []\n",
    "#         for token in doc:\n",
    "#             book[idx][\"POS_tags\"].append({\n",
    "#                 'text' : token.text,\n",
    "#                 'lemma' : token.lemma_,\n",
    "#                 'pos' : token.pos_, \n",
    "#                 'tag' : token.tag_, \n",
    "#                 'dep' : token.dep_})\n",
    "#     time_end = time.time()\n",
    "#     print(\"Parsing book took \", round(time_end - time_start, 2), \" secs\")\n",
    "#     return book, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book, characters = create_book_struct(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_book_to_disk(book, outFileName):\n",
    "#     with open (os.path.join(dataPath, outFileName), 'w') as outFile:\n",
    "#         json.dump(book, outFile, separators=(',', ':'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_book_to_disk(book, outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only get top n most mentioned characters. \n",
    "# # Probably we should replace this with some kind of \n",
    "# # filter based on the min number of mentions\n",
    "# most_common_n = 20\n",
    "# top_characters = characters.most_common()[:most_common_n]\n",
    "# top_characters = [top_character[0] for top_character in top_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('''\n",
    "# I am a rather elderly man.  The nature of my avocations for the last\n",
    "# thirty years has brought me into more than ordinary contact with what\n",
    "# would seem an interesting and somewhat singular set of men, of whom as\n",
    "# yet nothing that I know of has ever been written:--I mean the\n",
    "# law-copyists or scriveners.  I have known very many of them,\n",
    "# professionally and privately, and if I pleased, could relate divers\n",
    "# histories, at which good-natured gentlemen might smile, and sentimental\n",
    "# souls might weep.  But I waive the biographies of all other scriveners\n",
    "# for a few passages in the life of Bartleby, who was a scrivener of the\n",
    "# strangest I ever saw or heard of.  While of other law-copyists I might\n",
    "# write the complete life, of Bartleby nothing of that sort can be done.\n",
    "# I believe that no materials exist for a full and satisfactory biography\n",
    "# of this man.  It is an irreparable loss to literature.  Bartleby was one\n",
    "# of those beings of whom nothing is ascertainable, except from the\n",
    "# original sources, and in his case those are very small.  What my own\n",
    "# astonished eyes saw of Bartleby, _that_ is all I know of him, except,\n",
    "# indeed, one vague report which will appear in the sequel.\n",
    "# ''')\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
