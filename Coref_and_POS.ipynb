{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dmac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f5948b66910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp, blacklist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "\n",
    "# nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A class reresenting a mention of some entity. \n",
    "It consists of a text string, starting and ending indices\n",
    "of the text span and a paragraph and sentence id. \n",
    "'''\n",
    "class Mention:\n",
    "    def __init__(self, text, start, end, par_id, sent_id):\n",
    "        self.text =  text\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.par_id = par_id\n",
    "        self.sent_id = sent_id\n",
    "        self.POS = []\n",
    "        self.deps = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Mention in par \" + str(self.par_id) + \" Sentence \" \\\n",
    "              + str(self.sent_id) + \" text:\\n\" + self.text\n",
    "        return rep\n",
    "\n",
    "'''\n",
    "A class reresenting a character. \n",
    "It consists of a list of mentions, a set of aliases, \n",
    "a list of verbs that the character is the actor of (agent),\n",
    "a list of verbs that the character is the receiver of (patient),\n",
    "a list of adjectives that the character is described with (description).  \n",
    "'''\n",
    "class Character:\n",
    "    def __init__(self, book, mainName):\n",
    "        self.book = book\n",
    "        self.mainName = mainName\n",
    "        self.mentions = []\n",
    "        self.unique_sents = {}\n",
    "        self.aliases = set()\n",
    "        self.agent = {}\n",
    "        self.patient = []\n",
    "        self.description = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Character: \" + self.mainName + \"\\n\"\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Add a mention of the character to a list of mentions.\n",
    "    '''\n",
    "    def update_mention(self, mention):\n",
    "        self.mentions.append(mention)\n",
    "        self.aliases.update([mention.text.lower()])\n",
    "        \n",
    "    '''\n",
    "    Match POS tags with character mentions\n",
    "    '''\n",
    "    def get_POS(self):\n",
    "        for mention in self.mentions:\n",
    "            span = range(mention.start, mention.end)\n",
    "            for loc in span:\n",
    "                mention.POS.append(self.book.pars[mention.par_id].POS_tags[loc])\n",
    "                mention.deps.update([mention.POS[-1]['dep']])\n",
    "                \n",
    "    def get_unique_sent_mentions(self):\n",
    "        for mention in self.mentions:\n",
    "            self.unique_sents[(mention.par_id, mention.sent_id)] = \\\n",
    "            self.book.pars[mention.par_id].sents[mention.sent_id].text\n",
    "                \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the nsubj.\n",
    "    '''\n",
    "    def get_agent_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if 'nsubj' in mention.deps:\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['text']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.agent[global_loc] = verb\n",
    "                    \n",
    "        \n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, book, par, text, sent_id, bound):\n",
    "        # Parent objects\n",
    "        self.book = book\n",
    "        self.par = par\n",
    "        # starting and ending boundary for the sentence (relative to the paragraph)\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Global starting and ending indices\n",
    "        self.globalStart = self.par.start + self.start\n",
    "        self.globalEnd = self.par.start + self.end \n",
    "        self.text = text\n",
    "        self.sent_id = sent_id\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par.par_id) \\\n",
    "        + \"\\nSentence \" + str(self.sent_id) + \" start \" + str(self.start) \\\n",
    "        + \" end \" + str(self.end) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "        \n",
    "'''\n",
    "A class for a paragraph.\n",
    "'''\n",
    "class Paragraph:\n",
    "    def __init__(self, book, text, par_id, bound):\n",
    "        # Starting and ending indices for the paragraph\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Refers to book parent object\n",
    "        self.book = book\n",
    "        # paragraph text\n",
    "        self.text = text\n",
    "        # paragraph index\n",
    "        self.par_id = par_id\n",
    "        # bool idnicating whether coref wroked\n",
    "        self.has_coref = False\n",
    "        # coref cluster list\n",
    "        self.coref_clusts = []\n",
    "        # list of sentences\n",
    "        self.sents = []\n",
    "        self.sent_bounds = []\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par_id) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Split text into sentences\n",
    "    '''\n",
    "    def parse_into_sentences(self):\n",
    "        doc = nlp(self.text)\n",
    "        sentences = [sent for sent in doc.sents]\n",
    "        self.sents = [Sentence(self.book, self, sent.text, \n",
    "                               sent_id, (sent.start, sent.end)) \n",
    "                      for sent_id, sent in enumerate(sentences)]\n",
    "        \n",
    "    def run_coref_POS(self):\n",
    "        # Get coreference tags:\n",
    "        doc = nlp(self.text)\n",
    "        # Update whether the coref parser returned results\n",
    "        self.has_coref = doc._.has_coref\n",
    "        if doc._.has_coref:\n",
    "            self.coref_clusts = doc._.coref_clusters\n",
    "            self.book.parse_coref_clusts(self)\n",
    "        # Get POS tags:\n",
    "\n",
    "        # this uses the pretrained BERT model:\n",
    "        # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "        # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "        self.POS_tags = []\n",
    "        # Add POS tags to list for paragraph and for each individual sentence\n",
    "        for token_id in range(len(doc)):\n",
    "            token = doc[token_id]\n",
    "            sent_id = 0\n",
    "            for sent in self.sents:\n",
    "                if ((token_id >= sent.start) and (token_id <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "            self.sents[sent_id].POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "            self.POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "\n",
    "'''\n",
    "A class representing a book.\n",
    "'''        \n",
    "class Book:\n",
    "    def __init__(self, dataPath, fileName):\n",
    "        self.dataPath = dataPath\n",
    "        self.fileName = fileName\n",
    "        self.text = ''\n",
    "        self.characters = {}\n",
    "        self.pars = []\n",
    "        self.read_file()\n",
    "    \n",
    "    '''\n",
    "    Read the text of the book from a txt file.\n",
    "    '''\n",
    "    def read_file(self):\n",
    "        with open(os.path.join(self.dataPath, self.fileName), \"r\") as txtFile:\n",
    "            self.text = txtFile.read()\n",
    "        \n",
    "    '''\n",
    "    Break the text into paragraphs.\n",
    "    '''\n",
    "    def parse_into_pars(self):\n",
    "        # split on newlines followed by space\n",
    "        pars = re.split('\\n\\s', self.text)   \n",
    "        par_bounds = [0]\n",
    "        par_bounds += [m.start(0) for m in re.finditer('\\n\\s', self.text)]\n",
    "        par_bounds.append(len(self.text) - 1)\n",
    "        # Replace newline chars\n",
    "        pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "        # Remove empty pars\n",
    "        pars = [par for par in pars if len(par) > 0]\n",
    "        # Convert each paragraph into a Paragraph\n",
    "        self.pars = [Paragraph(self, par, par_id, (par_bounds[par_id],\n",
    "                                                   par_bounds[par_id+1])) \n",
    "                     for par_id, par in enumerate(pars)]\n",
    "        \n",
    "    '''\n",
    "    Parse the coreference clusters returned from the parsing of a paragraph\n",
    "    '''\n",
    "    def parse_coref_clusts(self, par):\n",
    "        clustList = par.coref_clusts\n",
    "        # Iterate over the coreference clusters\n",
    "        for idx, cluster in enumerate(clustList):\n",
    "            # get the main cluster identity\n",
    "            mainSpan = cluster.main.text.lower()\n",
    "            # If a character object does not yet exist, create one\n",
    "            if mainSpan not in self.characters:\n",
    "                character = Character(self, mainSpan)\n",
    "                # Add it to the dict of characters\n",
    "                self.characters[mainSpan] = character\n",
    "            # Otherwsie find the character referred to here\n",
    "            else:\n",
    "                character = self.characters[mainSpan] \n",
    "            for mention in cluster.mentions:\n",
    "                # figure out which sentence the mention belongs to\n",
    "                sent_id = 0\n",
    "                for sent in par.sents:\n",
    "                    if ((mention.start >= sent.start) and (mention.end <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "                # create a mention object and add it to the character object\n",
    "                mention = Mention(mention.text, mention.start, mention.end, par.par_id, sent_id)\n",
    "                character.update_mention(mention)\n",
    "                \n",
    "    def parse_text(self):\n",
    "        self.parse_into_pars()\n",
    "        for par in self.pars:\n",
    "            par.parse_into_sentences()\n",
    "            par.run_coref_POS()\n",
    "        for characterName, character in self.characters.items():\n",
    "            character.get_POS()\n",
    "            character.get_agent_verbs()\n",
    "            character.get_unique_sent_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths and fileNames:\n",
    "dataPath = \"../../Data/\"\n",
    "fileName = \"Herman Melville___Bartleby, The Scrivener.txt\"\n",
    "stopwordsFileName = \"StopWords/jockers.stopwords\"\n",
    "outFileName = 'Herman Melville___Bartleby, The Scrivener_sentences.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Book(dataPath, fileName)\n",
    "b.parse_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in b.characters:\n",
    "    for loc, sent in character.unique_sents.items():\n",
    "        emote = get_emotion(sent)\n",
    "        print(loc, emote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4,\n",
       "  6): 'I do not speak it in vanity, but simply record the fact, that I was not unemployed in my profession by the late John Jacob Astor; a name which, I admit, I love to repeat, for it hath a rounded and orbicular sound to it, and rings like unto bullion.  ',\n",
       " (14,\n",
       "  0): 'It was fortunate for me that, owing to its peculiar cause--indigestion--the irritability and consequent nervousness of Nippers, were mainly observable in the morning, while in the afternoon he was comparatively mild.  ',\n",
       " (84, 0): 'Shall I acknowledge it?  ',\n",
       " (84,\n",
       "  1): 'The conclusion of this whole business was, that it soon became a fixed fact of my chambers, that a pale young scrivener, by the name of Bartleby, and a desk there; that he copied for me at the usual rate of four cents a folio (one hundred words); but he was permanently exempt from examining the work done by him, that duty being transferred to Turkey and Nippers, one of compliment doubtless to their superior acuteness; moreover, said Bartleby was never on any account to be dispatched on the most trivial errand of any sort; and that even if entreated to take upon him such a matter, it was generally understood that he would prefer not to--in other words, that he would refuse pointblank.',\n",
       " (148,\n",
       "  2): 'Masterly I call it, and such it must appear to any dispassionate thinker.  ',\n",
       " (167, 5): 'Will it be credited?  ',\n",
       " (167, 6): 'Ought I to acknowledge it?  ',\n",
       " (168, 5): 'At last I see it',\n",
       " (168,\n",
       "  6): ', I feel it; I penetrate to the predestinated purpose of my life.  ',\n",
       " (169,\n",
       "  1): 'But thus it often is, that the constant friction of illiberal minds wears out at last the best resolves of the more generous.  ',\n",
       " (169,\n",
       "  2): 'Though to be sure, when I reflected upon it, it was not strange that people entering my office should be struck by the peculiar aspect of the unaccountable Bartleby, and so be tempted to throw out some sinister observations concerning him.  '}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.characters[\"it\"].get_unique_sent_mentions()\n",
    "b.characters[\"it\"].unique_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2,\n",
       "  4): 'While of other law-copyists I might write the complete life, of Bartleby nothing of that sort can be done.',\n",
       " (2,\n",
       "  7): 'Bartleby was one of those beings of whom nothing is ascertainable, except from the original sources, and in his case those are very small.  ',\n",
       " (2,\n",
       "  8): 'What my own astonished eyes saw of Bartleby, _that_ is all I know of him, except, indeed, one vague report which will appear in the sequel.',\n",
       " (18,\n",
       "  2): 'I resolved to assign Bartleby a corner by the folding-doors, but on my side of them, so as to have this quiet man within easy call,',\n",
       " (18,\n",
       "  6): 'Still further to a satisfactory arrangement, I procured a high green folding screen, which might entirely isolate Bartleby from my sight, though not remove him from my voice.  ',\n",
       " (21,\n",
       "  3): 'In my haste and natural expectancy of instant compliance, I sat with my head bent over the original on my desk, and my right hand sideways, and somewhat nervously extended with the copy, so that immediately upon emerging from his retreat, Bartleby might snatch it and proceed to business without the least delay.',\n",
       " (27,\n",
       "  0): \"A few days after this, Bartleby concluded four lengthy documents, being quadruplicates of a week's testimony taken before me in my High Court of Chancery.  \",\n",
       " (27, 1): 'It became necessary to examine them.  ',\n",
       " (27, 2): 'It was an important suit, and great accuracy was imperative.  ',\n",
       " (27,\n",
       "  4): 'Accordingly Turkey, Nippers and Ginger Nut had taken their seats in a row, each with his document in hand, when I called to Bartleby to join this interesting group.',\n",
       " (51,\n",
       "  0): 'Meanwhile Bartleby sat in his hermitage, oblivious to every thing but his own peculiar business there.',\n",
       " (53, 5): 'Was Bartleby hot and spicy?  ',\n",
       " (53, 7): 'Ginger, then, had no effect upon Bartleby.',\n",
       " (69, 0): 'I closed the doors, and again advanced towards Bartleby.  ',\n",
       " (69, 3): 'I remembered that Bartleby never left the office.',\n",
       " (85, 0): 'As days passed on, I became considerably reconciled to Bartleby.  ',\n",
       " (85,\n",
       "  6): \"For it was exceeding difficult to bear in mind all the time those strange peculiarities, privileges, and unheard of exemptions, forming the tacit stipulations on Bartleby's part under which he remained in my office.  \",\n",
       " (85,\n",
       "  7): 'Now and then, in the eagerness of dispatching pressing business, I would inadvertently summon Bartleby, in a short, rapid tone, to put his finger, say, on the incipient tie of a bit of red tape with which I was about compressing some papers.  ',\n",
       " (88,\n",
       "  4): 'Furthermore, I was full of uneasiness as to what Bartleby could possibly be doing in my office in his shirt sleeves, and in an otherwise dismantled condition of a Sunday morning.  ',\n",
       " (88,\n",
       "  7): 'It was not to be thought of for a moment that Bartleby was an immoral person.  ',\n",
       " (88, 10): ', Bartleby was an eminently decorous person.  ',\n",
       " (88,\n",
       "  12): 'Besides, it was Sunday; and there was something about Bartleby that forbade the supposition that he would by any secular occupation',\n",
       " (89, 3): 'Bartleby was not to be seen.  ',\n",
       " (89,\n",
       "  4): 'I looked round anxiously, peeped behind his screen; but it was very plain that he was gone.  ',\n",
       " (89,\n",
       "  5): 'Upon more closely examining the place, I surmised that for an indefinite period Bartleby must have ate, dressed, and slept in my office, and that too without plate, mirror, or bed.  ',\n",
       " (89,\n",
       "  8): \"Yes, thought I, it is evident enough that Bartleby has been making his home here, keeping bachelor's hall all by himself.\",\n",
       " (89, 10): 'His poverty is great; but his solitude, how horrible!  ',\n",
       " (89,\n",
       "  14): 'And here Bartleby makes his home; sole spectator of a solitude which he has seen all populous--a sort of innocent and transformed Marius brooding among the ruins of Carthage!',\n",
       " (110,\n",
       "  2): 'At last, familiarly drawing my chair behind his screen, I sat down and said: \"Bartleby, never mind then about revealing your history; but let me entreat you, as a friend, to comply as far as may be with the usages of this office.  ',\n",
       " (110, 4): 'so, Bartleby.\"',\n",
       " (121,\n",
       "  0): '\"I would prefer to be left alone here,\" said Bartleby, as if offended at being mobbed in his privacy.',\n",
       " (127,\n",
       "  0): 'The next day I noticed that Bartleby did nothing but stand at his window in his dead-wall revery.  ',\n",
       " (127,\n",
       "  1): 'Upon asking him why he did not write, he said that he had decided upon doing no more writing.',\n",
       " (134, 1): \"Whether Bartleby's eyes improved or not, I could not say.  \",\n",
       " (134, 3): 'But when I asked him if they did, he vouchsafed no answer.  ',\n",
       " (134, 4): 'At all events, he would do no copying.  ',\n",
       " (134,\n",
       "  5): 'At last, in reply to my urgings, he informed me that he had permanently given up copying.',\n",
       " (137,\n",
       "  11): \"Decently as I could, I told Bartleby that in six days' time he must unconditionally leave the office.  \",\n",
       " (137,\n",
       "  12): 'I warned him to take measures, in the interval, for procuring some other abode.  ',\n",
       " (137, 13): 'I offered to assist him in this endeavor,',\n",
       " (137, 14): 'if he himself would but take the first step towards a removal.  ',\n",
       " (137,\n",
       "  15): '\"And when you finally quit me, Bartleby,\" added I, \"I shall see that you go not away entirely unprovided.  ',\n",
       " (146,\n",
       "  3): 'you have removed your things from these offices, Bartleby, you will of course lock the door--since every one is now gone for the day',\n",
       " (146, 8): 'Good-bye, Bartleby, and fare you well.\"',\n",
       " (148,\n",
       "  4): 'There was no vulgar bullying, no bravado of any sort, no choleric hectoring, and striding to and fro across the apartment, jerking out vehement commands for Bartleby to bundle himself off with his beggarly traps.  ',\n",
       " (148,\n",
       "  6): 'Without loudly bidding Bartleby depart--as an inferior genius might have done',\n",
       " (149,\n",
       "  1): 'One moment I thought it would prove a miserable failure, and Bartleby would be found all alive at my office as usual; the next moment it seemed certain that I should see his chair empty.  ',\n",
       " (156,\n",
       "  7): 'Yes, as before I had prospectively assumed that Bartleby would depart, so now I might retrospectively assume that departed he was.  ',\n",
       " (156,\n",
       "  8): 'In the legitimate carrying out of this assumption, I might enter my office in a great hurry, and pretending not to see Bartleby at all, walk straight against him as if he were air.  ',\n",
       " (156,\n",
       "  10): 'It was hardly possible that Bartleby could withstand such an application of the doctrine of assumptions.  ',\n",
       " (156, 12): 'I resolved to argue the matter over with him again.',\n",
       " (166,\n",
       "  0): 'But when this old Adam of resentment rose in me and tempted me concerning Bartleby, I grappled him and threw him.  ',\n",
       " (167,\n",
       "  2): 'Bartleby, of his own free accord, would emerge from his hermitage, and take up some decided line of march in the direction of the door.  ',\n",
       " (168,\n",
       "  2): 'Gradually I slid into the persuasion that these troubles of mine touching the scrivener, had been all predestinated from eternity, and Bartleby was billeted upon me for some mysterious purpose of an all-wise Providence, which it was not for a mere mortal like me to fathom.  ',\n",
       " (168, 3): 'Yes, Bartleby, stay there behind your screen, thought I',\n",
       " (170,\n",
       "  0): \"Also, when a Reference was going on, and the room full of lawyers and witnesses and business was driving fast; some deeply occupied legal gentleman present, seeing Bartleby wholly unemployed, would request him to run round to his (the legal gentleman's) office and fetch some papers for him.  \",\n",
       " (170,\n",
       "  1): 'Thereupon, Bartleby would tranquilly decline, and yet remain idle as before.  ',\n",
       " (170,\n",
       "  6): 'And as the idea came upon me of his possibly turning out a long-lived man, and keep occupying my chambers, and denying my authority; and perplexing my visitors; and scandalizing my professional reputation; and casting a general gloom over the premises; keeping soul and body together to the last upon his savings (for doubtless he spent but half a dime a day), and in the end perhaps outlive me, and claim possession of my office by right of his perpetual occupancy: as all these dark anticipations crowded upon me more and more, and my friends continually intruded their relentless remarks upon the apparition in my room; a great change was wrought in me.  ',\n",
       " (171,\n",
       "  0): 'Ere revolving any complicated project, however, adapted to this end, I first simply suggested to Bartleby the propriety of his permanent departure.  ',\n",
       " (171,\n",
       "  1): 'In a calm and serious tone, I commended the idea to his careful and mature consideration.  ',\n",
       " (171,\n",
       "  2): 'But having taken three days to meditate upon it, he apprised me that his original determination remained the same in short, that he still preferred to abide with me.',\n",
       " (222,\n",
       "  0): '\"It was not I that brought you here, Bartleby,\" said I, keenly pained at his implied suspicion.  '}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.characters[\"bartleby\"].get_unique_sent_mentions()\n",
    "b.characters[\"bartleby\"].unique_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4) anger\n",
      "(2, 7) fear\n",
      "(2, 8) surprise\n",
      "(18, 2) joy\n",
      "(18, 6) joy\n",
      "(21, 3) fear\n",
      "(27, 0) anger\n",
      "(27, 1) fear\n",
      "(27, 2) joy\n",
      "(27, 4) joy\n",
      "(51, 0) sadness\n",
      "(53, 5) love\n",
      "(53, 7) anger\n",
      "(69, 0) anger\n",
      "(69, 3) joy\n",
      "(85, 0) joy\n",
      "(85, 6) fear\n",
      "(85, 7) joy\n",
      "(88, 4) sadness\n",
      "(88, 7) anger\n",
      "(88, 10) joy\n",
      "(88, 12) anger\n",
      "(89, 3) fear\n",
      "(89, 4) fear\n",
      "(89, 5) fear\n",
      "(89, 8) joy\n",
      "(89, 10) sadness\n",
      "(89, 14) joy\n",
      "(110, 2) joy\n",
      "(110, 4) fear\n",
      "(121, 0) anger\n",
      "(127, 0) anger\n",
      "(127, 1) anger\n",
      "(134, 1) joy\n",
      "(134, 3) fear\n",
      "(134, 4) anger\n",
      "(134, 5) anger\n",
      "(137, 11) anger\n",
      "(137, 12) fear\n",
      "(137, 13) joy\n",
      "(137, 14) anger\n",
      "(137, 15) anger\n",
      "(146, 3) sadness\n",
      "(146, 8) sadness\n",
      "(148, 4) anger\n",
      "(148, 6) joy\n",
      "(149, 1) sadness\n",
      "(156, 7) sadness\n",
      "(156, 8) anger\n",
      "(156, 10) fear\n",
      "(156, 12) anger\n",
      "(166, 0) anger\n",
      "(167, 2) joy\n",
      "(168, 2) fear\n",
      "(168, 3) joy\n",
      "(170, 0) anger\n",
      "(170, 1) sadness\n",
      "(170, 6) fear\n",
      "(171, 0) anger\n",
      "(171, 1) joy\n",
      "(171, 2) joy\n",
      "(222, 0) sadness\n"
     ]
    }
   ],
   "source": [
    "for loc, sent in b.characters[\"bartleby\"].unique_sents.items():\n",
    "    #print(sent)\n",
    "    emote = get_emotion(sent)\n",
    "    print(loc, emote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 7, 183): 'was',\n",
       " (21, 3, 123): 'sat',\n",
       " (27, 0, 7): 'concluded',\n",
       " (27, 1, 31): 'became',\n",
       " (27, 2, 39): 'was',\n",
       " (51, 0, 2): 'sat',\n",
       " (53, 5, 107): 'Was',\n",
       " (69, 3, 33): 'remembered',\n",
       " (88, 4, 141): 'was',\n",
       " (88, 7, 192): 'was',\n",
       " (88, 10, 228): 'was',\n",
       " (89, 3, 39): 'was',\n",
       " (89, 5, 75): 'surmised',\n",
       " (89, 8, 185): 'thought',\n",
       " (89, 14, 301): 'makes',\n",
       " (110, 2, 86): 'sat',\n",
       " (121, 0, 11): 'said',\n",
       " (127, 0, 4): 'noticed',\n",
       " (127, 1, 32): 'said',\n",
       " (134, 3, 41): 'vouchsafed',\n",
       " (134, 4, 52): 'do',\n",
       " (134, 5, 67): 'informed',\n",
       " (137, 11, 182): 'told',\n",
       " (137, 14, 230): 'take',\n",
       " (148, 4, 67): 'was',\n",
       " (156, 7, 163): 'assumed',\n",
       " (156, 8, 192): 'enter',\n",
       " (156, 10, 239): 'was',\n",
       " (167, 2, 51): 'emerge',\n",
       " (170, 0, 36): 'request',\n",
       " (170, 1, 63): 'decline',\n",
       " (170, 6, 143): 'came',\n",
       " (171, 2, 57): 'apprised'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.characters[\"bartleby\"].agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_sent_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is some first pass testing that is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Split text into sentences\n",
    "# '''\n",
    "# def parse_into_sentences(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     sentences = re.split('\\.|!|\\?', doc)   \n",
    "#     sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "#     return sentences\n",
    "# '''\n",
    "# Split text into paragraphs\n",
    "# '''\n",
    "# def parse_into_pars(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     pars = re.split('\\n\\s', doc)   \n",
    "#     pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "#     return pars\n",
    "# '''\n",
    "# Create list of stop words (currently not used)\n",
    "# '''\n",
    "# def read_stopwords(filename):\n",
    "#     stopwords={}\n",
    "#     with open(filename) as file:\n",
    "#         for line in file:\n",
    "#             stopwords[line.rstrip()]=1\n",
    "#     return stopwords\n",
    "\n",
    "# '''\n",
    "# Clean sentences by removing trailing punctuation on words, \n",
    "# and converting to lowercase\n",
    "# '''\n",
    "# def clean_sentences(sentences, charsTOStrip = '\\\"\\', '):\n",
    "#     texts = [\n",
    "#         [word.strip(charsTOStrip) for word in sentence.lower().split()]\n",
    "#         for sentence in sentences]\n",
    "#     return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Convert list of neuralcoref.neuralcoref.Cluster objects \n",
    "# to list of dicts so that we can serialize it. \n",
    "# '''\n",
    "# def convert_clust_to_list_dict(clustList):\n",
    "#     dictList = []\n",
    "#     characters = {character : {\n",
    "#     \"aliases\" : set(),\n",
    "#     \"agent\" : [],\n",
    "#     \"patient\" : [],\n",
    "#     \"description\" : [],\n",
    "#     } for character in top_characters}\n",
    "    \n",
    "#     for idx, cluster in enumerate(clustList):\n",
    "#         mainSpan = cluster.main\n",
    "#         dictList.append({mainSpan.text.lower() : [{'start' : mention.start, 'end': mention.end, 'text' : mention.text}\n",
    "#                                            for mention in cluster.mentions]})\n",
    "#         characters[mainSpan.text.lower()] += 1\n",
    "#     return dictList, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating book data structure that we will use.\n",
    "# # For now it is just a dict, but maybe we can make it a class\n",
    "# def create_book_struct(fileName):\n",
    "    \n",
    "#     characters = collections.Counter()\n",
    "    \n",
    "#     time_start = time.time()\n",
    "\n",
    "#     # break file into paragraphs\n",
    "#     pars = parse_into_pars(fileName)\n",
    "#     #parse_into_sentences(fileName)\n",
    "\n",
    "#     # initialize book obj\n",
    "#     book = {}\n",
    "#     # Iterate over each sentence and \n",
    "#     for idx, sentence in enumerate(sentences):\n",
    "\n",
    "#         # Get sentence id and text\n",
    "#         book[idx] = {}\n",
    "#         book[idx]['text'] = sentence\n",
    "\n",
    "#         # Get coreference tags:\n",
    "#         doc = nlp(sentence)\n",
    "#         book[idx]['has_coref'] = doc._.has_coref\n",
    "#         if doc._.has_coref:\n",
    "#             coref_clusts, chars = convert_clust_to_list_dict(doc._.coref_clusters)\n",
    "#             book[idx]['coref_clusts'] = coref_clusts\n",
    "            \n",
    "#             characters += chars\n",
    "#         else:\n",
    "#             book[idx]['coref_clusts'] = []\n",
    "\n",
    "#         # Get POS tags:\n",
    "\n",
    "#         # this uses the pretrained BERT model:\n",
    "#         # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "#         # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "#         book[idx][\"POS_tags\"] = []\n",
    "#         for token in doc:\n",
    "#             book[idx][\"POS_tags\"].append({\n",
    "#                 'text' : token.text,\n",
    "#                 'lemma' : token.lemma_,\n",
    "#                 'pos' : token.pos_, \n",
    "#                 'tag' : token.tag_, \n",
    "#                 'dep' : token.dep_})\n",
    "#     time_end = time.time()\n",
    "#     print(\"Parsing book took \", round(time_end - time_start, 2), \" secs\")\n",
    "#     return book, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book, characters = create_book_struct(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_book_to_disk(book, outFileName):\n",
    "#     with open (os.path.join(dataPath, outFileName), 'w') as outFile:\n",
    "#         json.dump(book, outFile, separators=(',', ':'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_book_to_disk(book, outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only get top n most mentioned characters. \n",
    "# # Probably we should replace this with some kind of \n",
    "# # filter based on the min number of mentions\n",
    "# most_common_n = 20\n",
    "# top_characters = characters.most_common()[:most_common_n]\n",
    "# top_characters = [top_character[0] for top_character in top_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('''\n",
    "# I am a rather elderly man.  The nature of my avocations for the last\n",
    "# thirty years has brought me into more than ordinary contact with what\n",
    "# would seem an interesting and somewhat singular set of men, of whom as\n",
    "# yet nothing that I know of has ever been written:--I mean the\n",
    "# law-copyists or scriveners.  I have known very many of them,\n",
    "# professionally and privately, and if I pleased, could relate divers\n",
    "# histories, at which good-natured gentlemen might smile, and sentimental\n",
    "# souls might weep.  But I waive the biographies of all other scriveners\n",
    "# for a few passages in the life of Bartleby, who was a scrivener of the\n",
    "# strangest I ever saw or heard of.  While of other law-copyists I might\n",
    "# write the complete life, of Bartleby nothing of that sort can be done.\n",
    "# I believe that no materials exist for a full and satisfactory biography\n",
    "# of this man.  It is an irreparable loss to literature.  Bartleby was one\n",
    "# of those beings of whom nothing is ascertainable, except from the\n",
    "# original sources, and in his case those are very small.  What my own\n",
    "# astonished eyes saw of Bartleby, _that_ is all I know of him, except,\n",
    "# indeed, one vague report which will appear in the sequel.\n",
    "# ''')\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb5738235264811bb2fbc0cc9847076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1208.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccae6fc78b3747fcb20d49820471628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=791656.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75ca588e84b4ef18312af8bf46da477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1786.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1a60bc7d0341d0ba1d0165d887222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_auto.py:781: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d27de3e16407c958b6cfa40b21cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=891692894.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 3922]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_emotion(text):\n",
    "    input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "\n",
    "    dec = [tokenizer.decode(ids) for ids in output]\n",
    "    label = dec[0]\n",
    "    return output\n",
    "\n",
    "get_emotion(\"i feel as if i havent blogged in ages are at least truly blogged i am doing an update cute\") # Output: 'joy'\n",
    "\n",
    "#get_emotion(\"i have a feeling i kinda lost my best friend\") # Output: 'sadness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0, 1): 'am',\n",
       " (2, 1, 52): 'mean',\n",
       " (2, 2, 63): 'known',\n",
       " (2, 3, 101): 'waive',\n",
       " (2, 4, 141): 'write',\n",
       " (2, 5, 157): 'believe',\n",
       " (2, 8, 215): 'saw',\n",
       " (7, 0, 11): 'had',\n",
       " (7, 6, 195): 'seemed',\n",
       " (7, 7, 222): 'are',\n",
       " (12, 13, 358): 'was',\n",
       " (12, 14, 402): 'have',\n",
       " (12, 18, 538): 'had',\n",
       " (12, 23, 642): 'reasoned',\n",
       " (12, 24, 654): 'was',\n",
       " (12, 26, 708): 'presented',\n",
       " (12, 27, 746): 'thought',\n",
       " (12, 29, 769): 'believe',\n",
       " (13, 0, 10): 'had',\n",
       " (13, 1, 20): 'was',\n",
       " (13, 4, 108): 'spread',\n",
       " (17, 0, 9): 'engaged',\n",
       " (17, 1, 25): 'sedate',\n",
       " (18, 0, 3): 'stated',\n",
       " (18, 1, 38): 'threw',\n",
       " (18, 2, 49): 'resolved',\n",
       " (18, 4, 91): 'placed',\n",
       " (18, 6, 193): 'procured',\n",
       " (20, 3, 74): 'imagine',\n",
       " (20, 4, 113): 'say',\n",
       " (22, 0, 6): 'sit',\n",
       " (22, 1, 60): 'replied',\n",
       " (23, 0, 1): 'sat',\n",
       " (23, 2, 43): 'assume',\n",
       " (23, 3, 61): 'prefer',\n",
       " (24, 0, 6): 'echoed',\n",
       " (24, 1, 27): 'mean',\n",
       " (24, 2, 30): 'Are',\n",
       " (24, 3, 48): 'take',\n",
       " (25, 0, 8): 'said',\n",
       " (26, 0, 1): 'looked',\n",
       " (26, 4, 51): 'been',\n",
       " (26, 5, 78): 'have',\n",
       " (26, 6, 100): 'stood',\n",
       " (26, 10, 147): 'concluded',\n",
       " (31, 0, 8): 'said',\n",
       " (31, 2, 24): 'held',\n",
       " (32, 0, 9): 'said',\n",
       " (33, 1, 29): 'advanced',\n",
       " (36, 0, 7): 'flown',\n",
       " (36, 2, 53): 'began',\n",
       " (38, 0, 8): 'replied',\n",
       " (38, 1, 18): 'seemed',\n",
       " (42, 0, 4): 'said',\n",
       " (42, 1, 16): 'Am',\n",
       " (45, 0, 2): 'think',\n",
       " (50, 1, 8): 'pondered',\n",
       " (50, 3, 24): 'determined',\n",
       " (54, 2, 73): 'regarded',\n",
       " (54, 8, 119): 'get',\n",
       " (54, 9, 133): 'are',\n",
       " (54, 11, 167): 'purchase',\n",
       " (54, 12, 204): 'prove',\n",
       " (54, 15, 235): 'goaded',\n",
       " (54, 16, 264): 'essayed',\n",
       " (55, 0, 4): 'said',\n",
       " (62, 0, 11): 'think',\n",
       " (64, 0, 7): 'said',\n",
       " (64, 1, 23): 'think',\n",
       " (67, 2, 39): 'go',\n",
       " (68, 0, 7): 'suppose',\n",
       " (68, 1, 24): 'pray',\n",
       " (69, 0, 1): 'closed',\n",
       " (69, 1, 13): 'felt',\n",
       " (69, 2, 24): 'burned',\n",
       " (69, 3, 33): 'remembered',\n",
       " (70, 0, 4): 'said',\n",
       " (74, 0, 1): 'staggered',\n",
       " (74, 2, 21): 'Was',\n",
       " (82, 0, 2): 'prefer',\n",
       " (83, 0, 7): 'said',\n",
       " (83, 1, 41): 'intended',\n",
       " (83, 2, 64): 'thought',\n",
       " (84, 0, 2): 'acknowledge',\n",
       " (85, 1, 55): 'made',\n",
       " (85, 3, 87): 'had',\n",
       " (85, 4, 97): 'felt',\n",
       " (85, 5, 111): 'be',\n",
       " (85, 7, 189): 'summon',\n",
       " (85, 8, 244): 'was',\n",
       " (85, 9, 289): 'tended',\n",
       " (87, 0, 29): 'thought',\n",
       " (87, 1, 44): 'had',\n",
       " (87, 2, 74): 'called',\n",
       " (87, 3, 155): 'added',\n",
       " (92, 0, 5): 'seek',\n",
       " (92, 2, 64): 'groped',\n",
       " (92, 3, 72): 'felt',\n",
       " (92, 5, 95): 'opened',\n",
       " (93, 0, 2): 'recalled',\n",
       " (93, 1, 17): 'remembered',\n",
       " (93, 3, 186): 'remembered',\n",
       " (95, 0, 3): 'accomplish',\n",
       " (95, 1, 22): 'disqualified',\n",
       " (95, 2, 34): 'walked',\n",
       " (95, 3, 53): 'put',\n",
       " (99, 0, 20): 'going',\n",
       " (108, 0, 12): 'said',\n",
       " (110, 0, 2): 'sat',\n",
       " (110, 1, 36): 'felt',\n",
       " (110, 2, 86): 'sat',\n",
       " (116, 0, 6): 'got',\n",
       " (116, 1, 30): 'trembled',\n",
       " (123, 2, 16): 'use',\n",
       " (123, 3, 33): 'but',\n",
       " (132, 0, 1): 'looked',\n",
       " (133, 1, 6): 'said',\n",
       " (133, 2, 15): 'hinted',\n",
       " (133, 4, 84): 'thought',\n",
       " (133, 6, 128): 'went',\n",
       " (134, 1, 18): 'say',\n",
       " (134, 2, 26): 'thought',\n",
       " (134, 3, 41): 'vouchsafed',\n",
       " (136, 0, 9): 'answered',\n",
       " (139, 0, 1): 'buttoned',\n",
       " (140, 0, 8): 'replied',\n",
       " (143, 0, 2): 'had',\n",
       " (143, 1, 17): 'restored',\n",
       " (144, 0, 4): 'said',\n",
       " (144, 2, 34): 'handed',\n",
       " (146, 0, 3): 'leave',\n",
       " (146, 2, 31): 'turned',\n",
       " (146, 5, 70): 'slip',\n",
       " (146, 6, 91): 'see',\n",
       " (146, 7, 122): 'fail',\n",
       " (148, 0, 11): 'got',\n",
       " (148, 1, 23): 'plume',\n",
       " (148, 2, 38): 'call',\n",
       " (148, 7, 129): 'assumed',\n",
       " (148, 8, 162): 'charmed',\n",
       " (148, 9, 176): 'had',\n",
       " (148, 14, 269): 'was',\n",
       " (149, 0, 4): 'walked',\n",
       " (149, 1, 23): 'thought',\n",
       " (149, 2, 62): 'kept',\n",
       " (149, 3, 78): 'saw',\n",
       " (150, 0, 10): 'said',\n",
       " (152, 0, 3): 'putting',\n",
       " (152, 1, 30): 'bore',\n",
       " (152, 2, 60): 'had',\n",
       " (152, 4, 86): 'passed',\n",
       " (153, 0, 6): 'was',\n",
       " (153, 1, 17): 'stood',\n",
       " (153, 4, 36): 'tried',\n",
       " (153, 7, 73): 'was',\n",
       " (153, 8, 131): 'occupied',\n",
       " (155, 0, 2): 'thunderstruck',\n",
       " (155, 1, 9): 'stood',\n",
       " (156, 1, 6): 'murmured',\n",
       " (156, 2, 43): 'went',\n",
       " (156, 3, 101): 'was',\n",
       " (156, 4, 122): 'think',\n",
       " (156, 6, 140): 'was',\n",
       " (156, 7, 163): 'assumed',\n",
       " (156, 8, 192): 'enter',\n",
       " (156, 12, 271): 'resolved',\n",
       " (157, 0, 4): 'said',\n",
       " (157, 2, 33): 'thought',\n",
       " (157, 3, 41): 'imagined',\n",
       " (157, 5, 80): 'added',\n",
       " (157, 6, 90): 'touched',\n",
       " (160, 0, 13): 'replied',\n",
       " (165, 0, 1): 'was',\n",
       " (165, 2, 74): 'was',\n",
       " (167, 0, 1): 'endeavored',\n",
       " (167, 1, 20): 'tried',\n",
       " (167, 7, 149): 'left',\n",
       " (168, 0, 3): 'passed',\n",
       " (168, 2, 45): 'slid',\n",
       " (168, 4, 133): 'feel',\n",
       " (168, 5, 148): 'see',\n",
       " (168, 6, 156): 'penetrate',\n",
       " (168, 7, 167): 'am',\n",
       " (169, 0, 1): 'believe',\n",
       " (169, 2, 77): 'was',\n",
       " (171, 0, 16): 'suggested',\n",
       " (171, 1, 35): 'commended',\n",
       " (172, 0, 3): 'do',\n",
       " (172, 1, 8): 'said',\n",
       " (172, 2, 25): 'do',\n",
       " (172, 3, 28): 'ought',\n",
       " (172, 4, 36): 'say',\n",
       " (172, 6, 62): 'shall',\n",
       " (172, 10, 113): 'do',\n",
       " (172, 11, 120): 'let',\n",
       " (173, 8, 111): 'have',\n",
       " (173, 11, 164): 'quit',\n",
       " (173, 12, 176): 'move',\n",
       " (174, 0, 7): 'addressed',\n",
       " (174, 1, 25): 'is',\n",
       " (174, 2, 34): 'propose',\n",
       " (174, 3, 52): 'tell',\n",
       " (176, 0, 24): 'removed',\n",
       " (176, 1, 30): 'Throughout',\n",
       " (176, 3, 76): 'stood',\n",
       " (177, 0, 3): 'entered',\n",
       " (178, 0, 9): 'going',\n",
       " (178, 1, 36): 'dropped',\n",
       " (180, 0, 1): 'thought',\n",
       " (181, 0, 5): 'replied',\n",
       " (183, 0, 9): 'said',\n",
       " (183, 1, 37): 'is',\n",
       " (185, 0, 5): 'inform',\n",
       " (185, 1, 10): 'know',\n",
       " (185, 2, 18): 'employed',\n",
       " (187, 0, 2): 'passed',\n",
       " (189, 0, 11): 'cried',\n",
       " (191, 0, 6): 'fell',\n",
       " (191, 1, 26): 'persisted',\n",
       " (191, 4, 86): 'considered',\n",
       " (196, 0, 8): 'are',\n",
       " (201, 1, 15): 'like',\n",
       " (203, 0, 12): 'rejoined',\n",
       " (205, 0, 17): 'am',\n",
       " (211, 0, 9): 'cried',\n",
       " (211, 1, 50): 'feel',\n",
       " (211, 3, 75): 'concluded',\n",
       " (211, 4, 101): 'leaving',\n",
       " (212, 0, 4): 'said',\n",
       " (214, 0, 1): 'answered',\n",
       " (214, 1, 51): 'perceived',\n",
       " (214, 2, 102): 'strove',\n",
       " (214, 3, 124): 'was',\n",
       " (214, 4, 137): 'was',\n",
       " (214, 5, 205): 'lived',\n",
       " (215, 0, 14): 'lay',\n",
       " (215, 1, 21): 'opened',\n",
       " (215, 3, 66): 'wished',\n",
       " (215, 5, 97): 'was',\n",
       " (215, 6, 115): 'led',\n",
       " (218, 0, 4): 'received',\n",
       " (218, 1, 31): 'stated',\n",
       " (218, 2, 53): 'assured',\n",
       " (218, 3, 76): 'narrated',\n",
       " (218, 5, 136): 'begged',\n",
       " (219, 1, 42): 'found',\n",
       " (221, 0, 13): 'want',\n",
       " (222, 0, 13): 'said',\n",
       " (223, 0, 9): 'replied',\n",
       " (224, 0, 18): 'accosted',\n",
       " (228, 0, 2): 'am',\n",
       " (231, 0, 5): 'said',\n",
       " (231, 1, 40): 'let',\n",
       " (236, 0, 11): 'said',\n",
       " (236, 1, 26): 'am',\n",
       " (236, 2, 34): 'saying',\n",
       " (238, 0, 10): 'said',\n",
       " (240, 1, 17): 'stop',\n",
       " (240, 4, 38): 'see'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.characters['i'].agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
