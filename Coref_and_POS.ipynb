{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dmac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f5948b66910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports \n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp, blacklist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "\n",
    "# nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A class reresenting a mention of some entity. \n",
    "It consists of a text string, starting and ending indices\n",
    "of the text span and a paragraph and sentence id. \n",
    "'''\n",
    "class Mention:\n",
    "    def __init__(self, text, start, end, par_id, sent_id):\n",
    "        self.text =  text\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.par_id = par_id\n",
    "        self.sent_id = sent_id\n",
    "        self.POS = []\n",
    "        self.deps = set()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Mention in par \" + str(self.par_id) + \" Sentence \" \\\n",
    "              + str(self.sent_id) + \" text:\\n\" + self.text\n",
    "        return rep\n",
    "\n",
    "'''\n",
    "A class reresenting a character. \n",
    "It consists of a list of mentions, a set of aliases, \n",
    "a list of verbs that the character is the actor of (agent),\n",
    "a list of verbs that the character is the receiver of (patient),\n",
    "a list of adjectives that the character is described with (description).  \n",
    "'''\n",
    "class Character:\n",
    "    def __init__(self, book, mainName):\n",
    "        self.book = book\n",
    "        self.mainName = mainName\n",
    "        self.mentions = []\n",
    "        self.unique_sents = {}\n",
    "        self.aliases = set()\n",
    "        self.agent = {}\n",
    "        self.patient = []\n",
    "        self.description = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = \"Character: \" + self.mainName + \"\\n\"\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Add a mention of the character to a list of mentions.\n",
    "    '''\n",
    "    def update_mention(self, mention):\n",
    "        self.mentions.append(mention)\n",
    "        self.aliases.update([mention.text.lower()])\n",
    "        \n",
    "    '''\n",
    "    Match POS tags with character mentions\n",
    "    '''\n",
    "    def get_POS(self):\n",
    "        for mention in self.mentions:\n",
    "            span = range(mention.start, mention.end)\n",
    "            for loc in span:\n",
    "                mention.POS.append(self.book.pars[mention.par_id].POS_tags[loc])\n",
    "                mention.deps.update([mention.POS[-1]['dep']])\n",
    "                \n",
    "    def get_unique_sent_mentions(self):\n",
    "        for mention in self.mentions:\n",
    "            self.unique_sents[(mention.par_id, mention.sent_id)] = \\\n",
    "            self.book.pars[mention.par_id].sents[mention.sent_id].text\n",
    "                \n",
    "    '''\n",
    "    Function to find the verbs in sentences in which the character is mentioned \n",
    "    as the nsubj.\n",
    "    '''\n",
    "    def get_agent_verbs(self):\n",
    "        for mention in self.mentions:\n",
    "            verb = None\n",
    "            if 'nsubj' in mention.deps:\n",
    "                sent_POS_parse = self.book.pars[mention.par_id].sents[mention.sent_id].POS_tags\n",
    "                for POS in sent_POS_parse:\n",
    "                    if POS['dep'] == 'ROOT':\n",
    "                        verb = POS['text']\n",
    "                        # Get location tuple (paragraph, sentence, local idx)\n",
    "                        global_loc = (mention.par_id, mention.sent_id, POS['loc'])\n",
    "                if verb:\n",
    "                    self.agent[global_loc] = verb\n",
    "                    \n",
    "        \n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, book, par, text, sent_id, bound):\n",
    "        # Parent objects\n",
    "        self.book = book\n",
    "        self.par = par\n",
    "        # starting and ending boundary for the sentence (relative to the paragraph)\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Global starting and ending indices\n",
    "        self.globalStart = self.par.start + self.start\n",
    "        self.globalEnd = self.par.start + self.end \n",
    "        self.text = text\n",
    "        self.sent_id = sent_id\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par.par_id) \\\n",
    "        + \"\\nSentence \" + str(self.sent_id) + \" start \" + str(self.start) \\\n",
    "        + \" end \" + str(self.end) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "        \n",
    "'''\n",
    "A class for a paragraph.\n",
    "'''\n",
    "class Paragraph:\n",
    "    def __init__(self, book, text, par_id, bound):\n",
    "        # Starting and ending indices for the paragraph\n",
    "        self.start = bound[0]\n",
    "        self.end = bound[1]\n",
    "        # Refers to book parent object\n",
    "        self.book = book\n",
    "        # paragraph text\n",
    "        self.text = text\n",
    "        # paragraph index\n",
    "        self.par_id = par_id\n",
    "        # bool idnicating whether coref wroked\n",
    "        self.has_coref = False\n",
    "        # coref cluster list\n",
    "        self.coref_clusts = []\n",
    "        # list of sentences\n",
    "        self.sents = []\n",
    "        self.sent_bounds = []\n",
    "        self.POS_tags = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        rep = self.book.fileName + \"\\nParagraph \" + str(self.par_id) + \"\\ntext:\\n\" + self.text\n",
    "        return rep\n",
    "    \n",
    "    '''\n",
    "    Split text into sentences\n",
    "    '''\n",
    "    def parse_into_sentences(self):\n",
    "        doc = nlp(self.text)\n",
    "        sentences = [sent for sent in doc.sents]\n",
    "        self.sents = [Sentence(self.book, self, sent.text, \n",
    "                               sent_id, (sent.start, sent.end)) \n",
    "                      for sent_id, sent in enumerate(sentences)]\n",
    "        \n",
    "    def run_coref_POS(self):\n",
    "        # Get coreference tags:\n",
    "        doc = nlp(self.text)\n",
    "        # Update whether the coref parser returned results\n",
    "        self.has_coref = doc._.has_coref\n",
    "        if doc._.has_coref:\n",
    "            self.coref_clusts = doc._.coref_clusters\n",
    "            self.book.parse_coref_clusts(self)\n",
    "        # Get POS tags:\n",
    "\n",
    "        # this uses the pretrained BERT model:\n",
    "        # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "        # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "        self.POS_tags = []\n",
    "        # Add POS tags to list for paragraph and for each individual sentence\n",
    "        for token_id in range(len(doc)):\n",
    "            token = doc[token_id]\n",
    "            sent_id = 0\n",
    "            for sent in self.sents:\n",
    "                if ((token_id >= sent.start) and (token_id <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "            self.sents[sent_id].POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "            self.POS_tags.append({\n",
    "                'loc' : token_id,\n",
    "                'text' : token.text,\n",
    "                'lemma' : token.lemma_,\n",
    "                'pos' : token.pos_, \n",
    "                'tag' : token.tag_, \n",
    "                'dep' : token.dep_})\n",
    "\n",
    "'''\n",
    "A class representing a book.\n",
    "'''        \n",
    "class Book:\n",
    "    def __init__(self, dataPath, fileName):\n",
    "        self.dataPath = dataPath\n",
    "        self.fileName = fileName\n",
    "        self.text = ''\n",
    "        self.characters = {}\n",
    "        self.char_mention_counts = collections.Counter()\n",
    "        self.top_characters = {}\n",
    "        self.pars = []\n",
    "        self.read_file()\n",
    "    \n",
    "    '''\n",
    "    Read the text of the book from a txt file.\n",
    "    '''\n",
    "    def read_file(self):\n",
    "        with open(os.path.join(self.dataPath, self.fileName), \"r\") as txtFile:\n",
    "            self.text = txtFile.read()\n",
    "        \n",
    "    '''\n",
    "    Break the text into paragraphs.\n",
    "    '''\n",
    "    def parse_into_pars(self):\n",
    "        # split on newlines followed by space\n",
    "        pars = re.split('\\n\\s', self.text)   \n",
    "        par_bounds = [0]\n",
    "        par_bounds += [m.start(0) for m in re.finditer('\\n\\s', self.text)]\n",
    "        par_bounds.append(len(self.text) - 1)\n",
    "        # Replace newline chars\n",
    "        pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "        # Remove empty pars\n",
    "        pars = [par for par in pars if len(par) > 0]\n",
    "        # Convert each paragraph into a Paragraph\n",
    "        self.pars = [Paragraph(self, par, par_id, (par_bounds[par_id],\n",
    "                                                   par_bounds[par_id+1])) \n",
    "                     for par_id, par in enumerate(pars)]\n",
    "        \n",
    "    '''\n",
    "    Parse the coreference clusters returned from the parsing of a paragraph\n",
    "    '''\n",
    "    def parse_coref_clusts(self, par):\n",
    "        clustList = par.coref_clusts\n",
    "        # Iterate over the coreference clusters\n",
    "        for idx, cluster in enumerate(clustList):\n",
    "            # get the main cluster identity\n",
    "            mainSpan = cluster.main.text.lower()\n",
    "            # If a character object does not yet exist, create one\n",
    "            if mainSpan not in self.characters:\n",
    "                character = Character(self, mainSpan)\n",
    "                # Add it to the dict of characters\n",
    "                self.characters[mainSpan] = character\n",
    "            # Otherwsie find the character referred to here\n",
    "            else:\n",
    "                character = self.characters[mainSpan] \n",
    "            for mention in cluster.mentions:\n",
    "                # figure out which sentence the mention belongs to\n",
    "                sent_id = 0\n",
    "                for sent in par.sents:\n",
    "                    if ((mention.start >= sent.start) and (mention.end <= sent.end)):\n",
    "                        sent_id = sent.sent_id\n",
    "                # create a mention object and add it to the character object\n",
    "                mention = Mention(mention.text, mention.start, mention.end, par.par_id, sent_id)\n",
    "                character.update_mention(mention)\n",
    "                self.char_mention_counts[character.mainName] += 1\n",
    "                \n",
    "    def get_top_characters(self, n=5):\n",
    "        # Get n most mentioned characters\n",
    "        self.top_characters = self.char_mention_counts.most_common()[:n]\n",
    "        self.top_characters = {character[0] : self.characters[character[0]]\n",
    "                               for character in self.top_characters}\n",
    "                \n",
    "    def parse_text(self):\n",
    "        self.parse_into_pars()\n",
    "        for par in self.pars:\n",
    "            par.parse_into_sentences()\n",
    "            par.run_coref_POS()\n",
    "        for characterName, character in self.characters.items():\n",
    "            character.get_POS()\n",
    "            character.get_agent_verbs()\n",
    "            character.get_unique_sent_mentions()\n",
    "        self.get_top_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths and fileNames:\n",
    "dataPath = \"../../Data/\"\n",
    "fileName = \"Herman Melville___Bartleby, The Scrivener.txt\"\n",
    "stopwordsFileName = \"StopWords/jockers.stopwords\"\n",
    "outFileName = 'Herman Melville___Bartleby, The Scrivener_sentences.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Book(dataPath, fileName)\n",
    "b.parse_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(text):\n",
    "    input_ids = tokenizer.encode(text + '</s>', return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=input_ids,\n",
    "               max_length=2)\n",
    "\n",
    "    dec = [tokenizer.decode(ids) for ids in output]\n",
    "    label = dec[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i (2, 0) fear\n",
      "i (2, 1) joy\n",
      "i (2, 2) sadness\n",
      "i (2, 3) fear\n",
      "i (2, 4) anger\n",
      "i (2, 5) joy\n",
      "i (2, 8) surprise\n",
      "i (7, 0) joy\n",
      "i (7, 3) joy\n",
      "i (7, 4) joy\n",
      "i (7, 6) joy\n",
      "i (7, 7) sadness\n",
      "i (7, 12) sadness\n",
      "i (12, 13) joy\n",
      "i (12, 14) anger\n",
      "i (12, 15) joy\n",
      "i (12, 17) joy\n",
      "i (12, 18) anger\n",
      "i (12, 22) anger\n",
      "i (12, 23) anger\n",
      "i (12, 24) joy\n",
      "i (12, 26) joy\n",
      "i (12, 27) fear\n",
      "i (12, 29) anger\n",
      "i (13, 0) anger\n",
      "i (13, 1) joy\n",
      "i (13, 4) anger\n",
      "i (17, 0) joy\n",
      "i (17, 1) anger\n",
      "i (18, 0) anger\n",
      "i (18, 1) joy\n",
      "i (18, 2) joy\n",
      "i (18, 4) anger\n",
      "i (18, 6) joy\n",
      "i (20, 3) joy\n",
      "i (20, 4) joy\n",
      "i (22, 0) anger\n",
      "i (22, 1) anger\n",
      "i (23, 0) surprise\n",
      "i (23, 1) anger\n",
      "i (23, 2) joy\n",
      "i (23, 3) joy\n",
      "i (24, 0) joy\n",
      "i (24, 1) anger\n",
      "i (24, 2) surprise\n",
      "i (24, 3) anger\n",
      "i (25, 0) anger\n",
      "i (26, 0) joy\n",
      "i (26, 4) anger\n",
      "i (26, 5) sadness\n",
      "i (26, 6) anger\n",
      "i (26, 9) fear\n",
      "i (26, 10) joy\n",
      "i (31, 0) anger\n",
      "i (31, 2) joy\n",
      "i (32, 0) anger\n",
      "i (33, 0) anger\n",
      "i (33, 1) fear\n",
      "i (36, 0) anger\n",
      "i (36, 1) fear\n",
      "i (36, 2) anger\n",
      "i (38, 0) joy\n",
      "i (38, 1) joy\n",
      "i (42, 0) anger\n",
      "i (42, 1) joy\n",
      "i (45, 0) anger\n",
      "i (50, 1) anger\n",
      "i (50, 2) anger\n",
      "i (50, 3) joy\n",
      "i (54, 2) joy\n",
      "i (54, 4) fear\n",
      "i (54, 7) joy\n",
      "i (54, 8) joy\n",
      "i (54, 9) anger\n",
      "i (54, 11) joy\n",
      "i (54, 12) fear\n",
      "i (54, 13) joy\n",
      "i (54, 14) anger\n",
      "i (54, 15) anger\n",
      "i (54, 16) anger\n",
      "i (54, 17) anger\n",
      "i (55, 0) joy\n",
      "i (62, 0) anger\n",
      "i (64, 0) anger\n",
      "i (64, 1) joy\n",
      "i (64, 2) anger\n",
      "i (67, 0) love\n",
      "i (67, 1) love\n",
      "i (67, 2) fear\n",
      "i (68, 0) anger\n",
      "i (68, 1) joy\n",
      "i (69, 0) anger\n",
      "i (69, 1) joy\n",
      "i (69, 2) anger\n",
      "i (69, 3) joy\n",
      "i (70, 0) joy\n",
      "i (70, 1) joy\n",
      "i (74, 0) fear\n",
      "i (74, 1) fear\n",
      "i (74, 2) anger\n",
      "i (82, 0) anger\n",
      "i (83, 0) joy\n",
      "i (83, 1) joy\n",
      "i (83, 2) fear\n",
      "i (84, 0) joy\n",
      "i (84, 1) joy\n",
      "i (85, 0) joy\n",
      "i (85, 1) joy\n",
      "i (85, 3) joy\n",
      "i (85, 4) joy\n",
      "i (85, 5) fear\n",
      "i (85, 6) fear\n",
      "i (85, 7) joy\n",
      "i (85, 8) sadness\n",
      "i (85, 9) anger\n",
      "i (87, 0) joy\n",
      "i (87, 1) fear\n",
      "i (87, 2) surprise\n",
      "i (87, 3) joy\n",
      "i (92, 0) anger\n",
      "i (92, 2) fear\n",
      "i (92, 3) anger\n",
      "i (92, 5) joy\n",
      "i (93, 0) joy\n",
      "i (93, 1) joy\n",
      "i (93, 3) fear\n",
      "i (95, 0) joy\n",
      "i (95, 1) sadness\n",
      "i (95, 2) fear\n",
      "i (95, 3) joy\n",
      "i (99, 0) joy\n",
      "i (108, 0) sadness\n",
      "i (110, 0) fear\n",
      "i (110, 1) fear\n",
      "i (110, 2) joy\n",
      "i (116, 0) joy\n",
      "i (116, 1) fear\n",
      "i (116, 3) fear\n",
      "i (123, 2) anger\n",
      "i (123, 3) joy\n",
      "i (132, 0) sadness\n",
      "i (132, 1) fear\n",
      "i (133, 0) anger\n",
      "i (133, 1) sadness\n",
      "i (133, 2) joy\n",
      "i (133, 4) anger\n",
      "i (133, 6) sadness\n",
      "i (134, 1) joy\n",
      "i (134, 2) fear\n",
      "i (134, 3) fear\n",
      "i (134, 5) anger\n",
      "i (136, 0) anger\n",
      "i (139, 0) anger\n",
      "i (140, 0) anger\n",
      "i (143, 0) joy\n",
      "i (143, 1) anger\n",
      "i (144, 0) anger\n",
      "i (144, 2) joy\n",
      "i (146, 0) fear\n",
      "i (146, 1) anger\n",
      "i (146, 2) joy\n",
      "i (146, 5) joy\n",
      "i (146, 6) sadness\n",
      "i (146, 7) joy\n",
      "i (148, 0) anger\n",
      "i (148, 1) joy\n",
      "i (148, 2) anger\n",
      "i (148, 3) joy\n",
      "i (148, 7) fear\n",
      "i (148, 8) joy\n",
      "i (148, 9) fear\n",
      "i (148, 11) joy\n",
      "i (148, 13) joy\n",
      "i (148, 14) joy\n",
      "i (149, 0) anger\n",
      "i (149, 1) sadness\n",
      "i (149, 2) anger\n",
      "i (149, 3) joy\n",
      "i (150, 0) fear\n",
      "i (152, 0) joy\n",
      "i (152, 1) joy\n",
      "i (152, 2) joy\n",
      "i (152, 3) joy\n",
      "i (152, 4) sadness\n",
      "i (153, 0) anger\n",
      "i (153, 1) joy\n",
      "i (153, 4) fear\n",
      "i (153, 6) joy\n",
      "i (153, 7) sadness\n",
      "i (153, 8) fear\n",
      "i (155, 0) surprise\n",
      "i (155, 1) sadness\n",
      "i (156, 1) fear\n",
      "i (156, 2) joy\n",
      "i (156, 3) anger\n",
      "i (156, 4) fear\n",
      "i (156, 6) joy\n",
      "i (156, 7) sadness\n",
      "i (156, 8) anger\n",
      "i (156, 12) anger\n",
      "i (157, 0) anger\n",
      "i (157, 1) sadness\n",
      "i (157, 2) joy\n",
      "i (157, 3) love\n",
      "i (157, 4) anger\n",
      "i (157, 5) joy\n",
      "i (157, 6) anger\n",
      "i (160, 0) anger\n",
      "i (165, 0) fear\n",
      "i (165, 1) sadness\n",
      "i (165, 2) anger\n",
      "i (165, 3) anger\n",
      "i (167, 0) sadness\n",
      "i (167, 1) joy\n",
      "i (167, 6) fear\n",
      "i (167, 7) sadness\n",
      "i (168, 0) joy\n",
      "i (168, 2) fear\n",
      "i (168, 3) joy\n",
      "i (168, 4) joy\n",
      "i (168, 5) joy\n",
      "i (168, 6) joy\n",
      "i (168, 7) joy\n",
      "i (168, 8) joy\n",
      "i (169, 0) joy\n",
      "i (169, 2) fear\n",
      "i (169, 3) anger\n",
      "i (171, 0) anger\n",
      "i (171, 1) joy\n",
      "i (171, 2) joy\n",
      "i (172, 0) fear\n",
      "i (172, 1) joy\n",
      "i (172, 2) fear\n",
      "i (172, 3) fear\n",
      "i (172, 4) anger\n",
      "i (172, 5) anger\n",
      "i (172, 6) anger\n",
      "i (172, 10) anger\n",
      "i (172, 11) anger\n",
      "i (173, 8) anger\n",
      "i (173, 11) anger\n",
      "i (173, 12) anger\n",
      "i (174, 0) joy\n",
      "i (174, 1) sadness\n",
      "i (174, 2) anger\n",
      "i (174, 3) joy\n",
      "i (176, 0) anger\n",
      "i (176, 1) fear\n",
      "i (176, 2) fear\n",
      "i (176, 3) anger\n",
      "i (177, 0) fear\n",
      "i (177, 1) joy\n",
      "i (178, 0) sadness\n",
      "i (178, 1) sadness\n",
      "i (180, 0) fear\n",
      "i (181, 0) sadness\n",
      "i (183, 0) sadness\n",
      "i (183, 1) anger\n",
      "i (185, 0) fear\n",
      "i (185, 1) fear\n",
      "i (185, 2) anger\n",
      "i (187, 0) joy\n",
      "i (189, 0) joy\n",
      "i (191, 0) anger\n",
      "i (191, 1) sadness\n",
      "i (191, 2) sadness\n",
      "i (191, 4) anger\n",
      "i (196, 0) fear\n",
      "i (201, 1) joy\n",
      "i (203, 0) anger\n",
      "i (205, 0) joy\n",
      "i (211, 0) sadness\n",
      "i (211, 1) joy\n",
      "i (211, 2) joy\n",
      "i (211, 3) fear\n",
      "i (211, 4) sadness\n",
      "i (212, 0) joy\n",
      "i (214, 0) anger\n",
      "i (214, 1) anger\n",
      "i (214, 2) joy\n",
      "i (214, 3) joy\n",
      "i (214, 4) fear\n",
      "i (214, 5) fear\n",
      "i (215, 0) anger\n",
      "i (215, 1) fear\n",
      "i (215, 2) anger\n",
      "i (215, 3) joy\n",
      "i (215, 4) anger\n",
      "i (215, 5) anger\n",
      "i (215, 6) joy\n",
      "i (218, 0) joy\n",
      "i (218, 1) joy\n",
      "i (218, 2) joy\n",
      "i (218, 3) joy\n",
      "i (218, 5) anger\n",
      "i (219, 1) fear\n",
      "i (221, 0) joy\n",
      "i (222, 0) sadness\n",
      "i (223, 0) anger\n",
      "i (224, 0) fear\n",
      "i (228, 0) anger\n",
      "i (228, 1) joy\n",
      "i (231, 0) joy\n",
      "i (231, 1) joy\n",
      "i (236, 0) anger\n",
      "i (236, 1) anger\n",
      "i (236, 2) anger\n",
      "i (238, 0) sadness\n",
      "i (240, 0) fear\n",
      "i (240, 1) anger\n",
      "i (240, 2) joy\n",
      "i (240, 4) joy\n",
      "his (7, 7) sadness\n",
      "his (7, 8) sadness\n",
      "his (7, 9) joy\n",
      "his (7, 10) fear\n",
      "his (7, 11) fear\n",
      "his (7, 12) sadness\n",
      "his (7, 13) anger\n",
      "his (7, 14) anger\n",
      "his (7, 15) anger\n",
      "his (7, 16) anger\n",
      "his (7, 17) joy\n",
      "his (7, 18) anger\n",
      "his (7, 19) anger\n",
      "his (7, 20) anger\n",
      "his (7, 21) joy\n",
      "his (12, 27) fear\n",
      "his (12, 29) anger\n",
      "his (12, 30) anger\n",
      "his (12, 31) anger\n",
      "his (12, 32) anger\n",
      "his (15, 9) anger\n",
      "his (15, 10) anger\n",
      "his (15, 11) anger\n",
      "his (15, 12) anger\n",
      "his (17, 0) joy\n",
      "his (21, 1) joy\n",
      "his (21, 2) fear\n",
      "his (21, 3) fear\n",
      "his (29, 0) fear\n",
      "his (50, 4) anger\n",
      "his (50, 5) anger\n",
      "his (52, 1) joy\n",
      "his (52, 2) joy\n",
      "his (52, 4) fear\n",
      "his (52, 5) anger\n",
      "his (52, 7) joy\n",
      "his (54, 1) anger\n",
      "his (54, 2) joy\n",
      "his (54, 5) anger\n",
      "his (54, 6) anger\n",
      "his (54, 7) joy\n",
      "his (54, 8) joy\n",
      "his (54, 9) anger\n",
      "his (54, 12) fear\n",
      "his (54, 15) anger\n",
      "his (61, 1) anger\n",
      "his (62, 0) anger\n",
      "his (63, 0) anger\n",
      "his (63, 1) fear\n",
      "his (85, 3) joy\n",
      "his (85, 4) joy\n",
      "his (87, 2) surprise\n",
      "his (87, 3) joy\n",
      "his (88, 2) joy\n",
      "his (88, 3) joy\n",
      "his (88, 4) sadness\n",
      "his (88, 8) fear\n",
      "his (88, 9) fear\n",
      "his (88, 11) anger\n",
      "his (88, 12) anger\n",
      "his (94, 9) sadness\n",
      "his (109, 1) anger\n",
      "his (109, 2) sadness\n",
      "his (155, 1) sadness\n",
      "his (166, 7) joy\n",
      "his (166, 10) joy\n",
      "his (206, 0) joy\n",
      "his (206, 1) anger\n",
      "his (211, 3) fear\n",
      "his (211, 4) sadness\n",
      "his (214, 1) anger\n",
      "his (219, 0) joy\n",
      "his (219, 1) fear\n",
      "his (233, 0) joy\n",
      "my (5, 0) joy\n",
      "my (5, 1) sadness\n",
      "my (5, 3) anger\n",
      "my (5, 4) joy\n",
      "my (6, 0) anger\n",
      "my (6, 3) joy\n",
      "my (6, 4) joy\n",
      "my (6, 5) fear\n",
      "my (11, 0) anger\n",
      "my (11, 1) sadness\n",
      "my (11, 3) joy\n",
      "my (12, 0) anger\n",
      "my (12, 1) anger\n",
      "my (15, 0) joy\n",
      "my (15, 1) joy\n",
      "my (15, 2) joy\n",
      "my (15, 7) anger\n",
      "my (15, 11) anger\n",
      "my (15, 12) anger\n",
      "my (15, 13) love\n",
      "my (16, 0) joy\n",
      "my (16, 2) anger\n",
      "my (16, 3) joy\n",
      "my (16, 4) joy\n",
      "my (21, 0) anger\n",
      "my (21, 1) joy\n",
      "my (21, 2) fear\n",
      "my (21, 3) fear\n",
      "my (86, 0) joy\n",
      "my (86, 1) fear\n",
      "my (86, 3) fear\n",
      "my (86, 5) fear\n",
      "my (88, 0) fear\n",
      "my (88, 2) joy\n",
      "my (88, 3) joy\n",
      "my (88, 4) sadness\n",
      "my (88, 6) anger\n",
      "my (88, 9) fear\n",
      "my (89, 0) fear\n",
      "my (89, 2) joy\n",
      "my (89, 4) fear\n",
      "my (89, 5) fear\n",
      "my (89, 7) sadness\n",
      "my (89, 8) joy\n",
      "my (89, 9) sadness\n",
      "my (90, 0) sadness\n",
      "my (90, 1) sadness\n",
      "my (90, 2) sadness\n",
      "my (90, 4) anger\n",
      "my (90, 5) joy\n",
      "my (90, 7) fear\n",
      "my (90, 8) fear\n",
      "my (94, 0) fear\n",
      "my (94, 1) sadness\n",
      "my (94, 8) fear\n",
      "my (94, 9) sadness\n",
      "my (179, 0) joy\n",
      "my (179, 1) fear\n",
      "my (179, 2) joy\n",
      "my (179, 4) fear\n",
      "my (239, 2) anger\n",
      "my (239, 3) anger\n",
      "my (239, 5) fear\n",
      "my (239, 7) sadness\n",
      "my (239, 8) fear\n",
      "bartleby (2, 4) anger\n",
      "bartleby (2, 7) fear\n",
      "bartleby (2, 8) surprise\n",
      "bartleby (18, 2) joy\n",
      "bartleby (18, 6) joy\n",
      "bartleby (21, 3) fear\n",
      "bartleby (27, 0) anger\n",
      "bartleby (27, 1) fear\n",
      "bartleby (27, 2) joy\n",
      "bartleby (27, 4) joy\n",
      "bartleby (51, 0) sadness\n",
      "bartleby (53, 5) love\n",
      "bartleby (53, 7) anger\n",
      "bartleby (69, 0) anger\n",
      "bartleby (69, 3) joy\n",
      "bartleby (85, 0) joy\n",
      "bartleby (85, 6) fear\n",
      "bartleby (85, 7) joy\n",
      "bartleby (88, 4) sadness\n",
      "bartleby (88, 7) anger\n",
      "bartleby (88, 10) joy\n",
      "bartleby (88, 12) anger\n",
      "bartleby (89, 3) fear\n",
      "bartleby (89, 4) fear\n",
      "bartleby (89, 5) fear\n",
      "bartleby (89, 8) joy\n",
      "bartleby (89, 10) sadness\n",
      "bartleby (89, 14) joy\n",
      "bartleby (110, 2) joy\n",
      "bartleby (110, 4) fear\n",
      "bartleby (121, 0) anger\n",
      "bartleby (127, 0) anger\n",
      "bartleby (127, 1) anger\n",
      "bartleby (134, 1) joy\n",
      "bartleby (134, 3) fear\n",
      "bartleby (134, 4) anger\n",
      "bartleby (134, 5) anger\n",
      "bartleby (137, 11) anger\n",
      "bartleby (137, 12) fear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bartleby (137, 13) joy\n",
      "bartleby (137, 14) anger\n",
      "bartleby (137, 15) anger\n",
      "bartleby (146, 3) sadness\n",
      "bartleby (146, 8) sadness\n",
      "bartleby (148, 4) anger\n",
      "bartleby (148, 6) joy\n",
      "bartleby (149, 1) sadness\n",
      "bartleby (156, 7) sadness\n",
      "bartleby (156, 8) anger\n",
      "bartleby (156, 10) fear\n",
      "bartleby (156, 12) anger\n",
      "bartleby (166, 0) anger\n",
      "bartleby (167, 2) joy\n",
      "bartleby (168, 2) fear\n",
      "bartleby (168, 3) joy\n",
      "bartleby (170, 0) anger\n",
      "bartleby (170, 1) sadness\n",
      "bartleby (170, 6) fear\n",
      "bartleby (171, 0) anger\n",
      "bartleby (171, 1) joy\n",
      "bartleby (171, 2) joy\n",
      "bartleby (222, 0) sadness\n",
      "he (11, 2) anger\n",
      "he (11, 3) joy\n",
      "he (12, 3) fear\n",
      "he (12, 4) joy\n",
      "he (12, 5) joy\n",
      "he (12, 7) anger\n",
      "he (12, 8) anger\n",
      "he (12, 9) sadness\n",
      "he (15, 6) joy\n",
      "he (40, 0) joy\n",
      "he (40, 1) joy\n",
      "he (53, 0) anger\n",
      "he (53, 1) fear\n",
      "he (53, 8) anger\n",
      "he (60, 0) anger\n",
      "he (80, 0) joy\n",
      "he (84, 1) joy\n",
      "he (85, 6) fear\n",
      "he (85, 7) joy\n",
      "he (93, 1) joy\n",
      "he (93, 2) sadness\n",
      "he (94, 0) fear\n",
      "he (106, 0) anger\n",
      "he (126, 0) fear\n",
      "he (126, 1) anger\n",
      "he (126, 2) anger\n",
      "he (137, 0) joy\n",
      "he (137, 1) sadness\n",
      "he (137, 3) anger\n",
      "he (137, 4) sadness\n",
      "he (137, 5) sadness\n",
      "he (137, 6) sadness\n",
      "he (137, 7) sadness\n",
      "he (137, 8) sadness\n",
      "he (147, 0) sadness\n",
      "he (148, 14) joy\n",
      "he (148, 15) joy\n",
      "he (153, 3) sadness\n",
      "he (153, 6) joy\n",
      "he (164, 0) sadness\n",
      "he (226, 0) anger\n",
      "he (226, 1) anger\n"
     ]
    }
   ],
   "source": [
    "for character in b.top_characters:\n",
    "    for loc, sent in b.top_characters[character].unique_sents.items():\n",
    "        emote = get_emotion(sent)\n",
    "        print(character, loc, emote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is some first pass testing that is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Split text into sentences\n",
    "# '''\n",
    "# def parse_into_sentences(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     sentences = re.split('\\.|!|\\?', doc)   \n",
    "#     sentences = [sentence.replace(\"\\n\", \" \") for sentence in sentences]\n",
    "#     return sentences\n",
    "# '''\n",
    "# Split text into paragraphs\n",
    "# '''\n",
    "# def parse_into_pars(fileName):\n",
    "#     with open(os.path.join(dataPath, fileName), \"r\") as txtFile:\n",
    "#         doc = txtFile.read()\n",
    "#     pars = re.split('\\n\\s', doc)   \n",
    "#     pars = [par.replace(\"\\n\", \" \") for par in pars]\n",
    "#     return pars\n",
    "# '''\n",
    "# Create list of stop words (currently not used)\n",
    "# '''\n",
    "# def read_stopwords(filename):\n",
    "#     stopwords={}\n",
    "#     with open(filename) as file:\n",
    "#         for line in file:\n",
    "#             stopwords[line.rstrip()]=1\n",
    "#     return stopwords\n",
    "\n",
    "# '''\n",
    "# Clean sentences by removing trailing punctuation on words, \n",
    "# and converting to lowercase\n",
    "# '''\n",
    "# def clean_sentences(sentences, charsTOStrip = '\\\"\\', '):\n",
    "#     texts = [\n",
    "#         [word.strip(charsTOStrip) for word in sentence.lower().split()]\n",
    "#         for sentence in sentences]\n",
    "#     return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Convert list of neuralcoref.neuralcoref.Cluster objects \n",
    "# to list of dicts so that we can serialize it. \n",
    "# '''\n",
    "# def convert_clust_to_list_dict(clustList):\n",
    "#     dictList = []\n",
    "#     characters = {character : {\n",
    "#     \"aliases\" : set(),\n",
    "#     \"agent\" : [],\n",
    "#     \"patient\" : [],\n",
    "#     \"description\" : [],\n",
    "#     } for character in top_characters}\n",
    "    \n",
    "#     for idx, cluster in enumerate(clustList):\n",
    "#         mainSpan = cluster.main\n",
    "#         dictList.append({mainSpan.text.lower() : [{'start' : mention.start, 'end': mention.end, 'text' : mention.text}\n",
    "#                                            for mention in cluster.mentions]})\n",
    "#         characters[mainSpan.text.lower()] += 1\n",
    "#     return dictList, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating book data structure that we will use.\n",
    "# # For now it is just a dict, but maybe we can make it a class\n",
    "# def create_book_struct(fileName):\n",
    "    \n",
    "#     characters = collections.Counter()\n",
    "    \n",
    "#     time_start = time.time()\n",
    "\n",
    "#     # break file into paragraphs\n",
    "#     pars = parse_into_pars(fileName)\n",
    "#     #parse_into_sentences(fileName)\n",
    "\n",
    "#     # initialize book obj\n",
    "#     book = {}\n",
    "#     # Iterate over each sentence and \n",
    "#     for idx, sentence in enumerate(sentences):\n",
    "\n",
    "#         # Get sentence id and text\n",
    "#         book[idx] = {}\n",
    "#         book[idx]['text'] = sentence\n",
    "\n",
    "#         # Get coreference tags:\n",
    "#         doc = nlp(sentence)\n",
    "#         book[idx]['has_coref'] = doc._.has_coref\n",
    "#         if doc._.has_coref:\n",
    "#             coref_clusts, chars = convert_clust_to_list_dict(doc._.coref_clusters)\n",
    "#             book[idx]['coref_clusts'] = coref_clusts\n",
    "            \n",
    "#             characters += chars\n",
    "#         else:\n",
    "#             book[idx]['coref_clusts'] = []\n",
    "\n",
    "#         # Get POS tags:\n",
    "\n",
    "#         # this uses the pretrained BERT model:\n",
    "#         # book[idx][\"POS_tags\"] = nlp_token_class(sentence)\n",
    "\n",
    "#         # For now we will use Spacy pos tagging because it gives us more fine-grained labels:\n",
    "#         book[idx][\"POS_tags\"] = []\n",
    "#         for token in doc:\n",
    "#             book[idx][\"POS_tags\"].append({\n",
    "#                 'text' : token.text,\n",
    "#                 'lemma' : token.lemma_,\n",
    "#                 'pos' : token.pos_, \n",
    "#                 'tag' : token.tag_, \n",
    "#                 'dep' : token.dep_})\n",
    "#     time_end = time.time()\n",
    "#     print(\"Parsing book took \", round(time_end - time_start, 2), \" secs\")\n",
    "#     return book, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book, characters = create_book_struct(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_book_to_disk(book, outFileName):\n",
    "#     with open (os.path.join(dataPath, outFileName), 'w') as outFile:\n",
    "#         json.dump(book, outFile, separators=(',', ':'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_book_to_disk(book, outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only get top n most mentioned characters. \n",
    "# # Probably we should replace this with some kind of \n",
    "# # filter based on the min number of mentions\n",
    "# most_common_n = 20\n",
    "# top_characters = characters.most_common()[:most_common_n]\n",
    "# top_characters = [top_character[0] for top_character in top_characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('''\n",
    "# I am a rather elderly man.  The nature of my avocations for the last\n",
    "# thirty years has brought me into more than ordinary contact with what\n",
    "# would seem an interesting and somewhat singular set of men, of whom as\n",
    "# yet nothing that I know of has ever been written:--I mean the\n",
    "# law-copyists or scriveners.  I have known very many of them,\n",
    "# professionally and privately, and if I pleased, could relate divers\n",
    "# histories, at which good-natured gentlemen might smile, and sentimental\n",
    "# souls might weep.  But I waive the biographies of all other scriveners\n",
    "# for a few passages in the life of Bartleby, who was a scrivener of the\n",
    "# strangest I ever saw or heard of.  While of other law-copyists I might\n",
    "# write the complete life, of Bartleby nothing of that sort can be done.\n",
    "# I believe that no materials exist for a full and satisfactory biography\n",
    "# of this man.  It is an irreparable loss to literature.  Bartleby was one\n",
    "# of those beings of whom nothing is ascertainable, except from the\n",
    "# original sources, and in his case those are very small.  What my own\n",
    "# astonished eyes saw of Bartleby, _that_ is all I know of him, except,\n",
    "# indeed, one vague report which will appear in the sequel.\n",
    "# ''')\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb5738235264811bb2fbc0cc9847076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1208.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccae6fc78b3747fcb20d49820471628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=791656.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75ca588e84b4ef18312af8bf46da477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1786.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1a60bc7d0341d0ba1d0165d887222e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/modeling_auto.py:781: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d27de3e16407c958b6cfa40b21cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=891692894.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmac/Documents/MIMS Coursework/Info 190 Comp Hum/Project/code/.env/lib/python3.8/site-packages/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 3922]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0, 1): 'am',\n",
       " (2, 1, 52): 'mean',\n",
       " (2, 2, 63): 'known',\n",
       " (2, 3, 101): 'waive',\n",
       " (2, 4, 141): 'write',\n",
       " (2, 5, 157): 'believe',\n",
       " (2, 8, 215): 'saw',\n",
       " (7, 0, 11): 'had',\n",
       " (7, 6, 195): 'seemed',\n",
       " (7, 7, 222): 'are',\n",
       " (12, 13, 358): 'was',\n",
       " (12, 14, 402): 'have',\n",
       " (12, 18, 538): 'had',\n",
       " (12, 23, 642): 'reasoned',\n",
       " (12, 24, 654): 'was',\n",
       " (12, 26, 708): 'presented',\n",
       " (12, 27, 746): 'thought',\n",
       " (12, 29, 769): 'believe',\n",
       " (13, 0, 10): 'had',\n",
       " (13, 1, 20): 'was',\n",
       " (13, 4, 108): 'spread',\n",
       " (17, 0, 9): 'engaged',\n",
       " (17, 1, 25): 'sedate',\n",
       " (18, 0, 3): 'stated',\n",
       " (18, 1, 38): 'threw',\n",
       " (18, 2, 49): 'resolved',\n",
       " (18, 4, 91): 'placed',\n",
       " (18, 6, 193): 'procured',\n",
       " (20, 3, 74): 'imagine',\n",
       " (20, 4, 113): 'say',\n",
       " (22, 0, 6): 'sit',\n",
       " (22, 1, 60): 'replied',\n",
       " (23, 0, 1): 'sat',\n",
       " (23, 2, 43): 'assume',\n",
       " (23, 3, 61): 'prefer',\n",
       " (24, 0, 6): 'echoed',\n",
       " (24, 1, 27): 'mean',\n",
       " (24, 2, 30): 'Are',\n",
       " (24, 3, 48): 'take',\n",
       " (25, 0, 8): 'said',\n",
       " (26, 0, 1): 'looked',\n",
       " (26, 4, 51): 'been',\n",
       " (26, 5, 78): 'have',\n",
       " (26, 6, 100): 'stood',\n",
       " (26, 10, 147): 'concluded',\n",
       " (31, 0, 8): 'said',\n",
       " (31, 2, 24): 'held',\n",
       " (32, 0, 9): 'said',\n",
       " (33, 1, 29): 'advanced',\n",
       " (36, 0, 7): 'flown',\n",
       " (36, 2, 53): 'began',\n",
       " (38, 0, 8): 'replied',\n",
       " (38, 1, 18): 'seemed',\n",
       " (42, 0, 4): 'said',\n",
       " (42, 1, 16): 'Am',\n",
       " (45, 0, 2): 'think',\n",
       " (50, 1, 8): 'pondered',\n",
       " (50, 3, 24): 'determined',\n",
       " (54, 2, 73): 'regarded',\n",
       " (54, 8, 119): 'get',\n",
       " (54, 9, 133): 'are',\n",
       " (54, 11, 167): 'purchase',\n",
       " (54, 12, 204): 'prove',\n",
       " (54, 15, 235): 'goaded',\n",
       " (54, 16, 264): 'essayed',\n",
       " (55, 0, 4): 'said',\n",
       " (62, 0, 11): 'think',\n",
       " (64, 0, 7): 'said',\n",
       " (64, 1, 23): 'think',\n",
       " (67, 2, 39): 'go',\n",
       " (68, 0, 7): 'suppose',\n",
       " (68, 1, 24): 'pray',\n",
       " (69, 0, 1): 'closed',\n",
       " (69, 1, 13): 'felt',\n",
       " (69, 2, 24): 'burned',\n",
       " (69, 3, 33): 'remembered',\n",
       " (70, 0, 4): 'said',\n",
       " (74, 0, 1): 'staggered',\n",
       " (74, 2, 21): 'Was',\n",
       " (82, 0, 2): 'prefer',\n",
       " (83, 0, 7): 'said',\n",
       " (83, 1, 41): 'intended',\n",
       " (83, 2, 64): 'thought',\n",
       " (84, 0, 2): 'acknowledge',\n",
       " (85, 1, 55): 'made',\n",
       " (85, 3, 87): 'had',\n",
       " (85, 4, 97): 'felt',\n",
       " (85, 5, 111): 'be',\n",
       " (85, 7, 189): 'summon',\n",
       " (85, 8, 244): 'was',\n",
       " (85, 9, 289): 'tended',\n",
       " (87, 0, 29): 'thought',\n",
       " (87, 1, 44): 'had',\n",
       " (87, 2, 74): 'called',\n",
       " (87, 3, 155): 'added',\n",
       " (92, 0, 5): 'seek',\n",
       " (92, 2, 64): 'groped',\n",
       " (92, 3, 72): 'felt',\n",
       " (92, 5, 95): 'opened',\n",
       " (93, 0, 2): 'recalled',\n",
       " (93, 1, 17): 'remembered',\n",
       " (93, 3, 186): 'remembered',\n",
       " (95, 0, 3): 'accomplish',\n",
       " (95, 1, 22): 'disqualified',\n",
       " (95, 2, 34): 'walked',\n",
       " (95, 3, 53): 'put',\n",
       " (99, 0, 20): 'going',\n",
       " (108, 0, 12): 'said',\n",
       " (110, 0, 2): 'sat',\n",
       " (110, 1, 36): 'felt',\n",
       " (110, 2, 86): 'sat',\n",
       " (116, 0, 6): 'got',\n",
       " (116, 1, 30): 'trembled',\n",
       " (123, 2, 16): 'use',\n",
       " (123, 3, 33): 'but',\n",
       " (132, 0, 1): 'looked',\n",
       " (133, 1, 6): 'said',\n",
       " (133, 2, 15): 'hinted',\n",
       " (133, 4, 84): 'thought',\n",
       " (133, 6, 128): 'went',\n",
       " (134, 1, 18): 'say',\n",
       " (134, 2, 26): 'thought',\n",
       " (134, 3, 41): 'vouchsafed',\n",
       " (136, 0, 9): 'answered',\n",
       " (139, 0, 1): 'buttoned',\n",
       " (140, 0, 8): 'replied',\n",
       " (143, 0, 2): 'had',\n",
       " (143, 1, 17): 'restored',\n",
       " (144, 0, 4): 'said',\n",
       " (144, 2, 34): 'handed',\n",
       " (146, 0, 3): 'leave',\n",
       " (146, 2, 31): 'turned',\n",
       " (146, 5, 70): 'slip',\n",
       " (146, 6, 91): 'see',\n",
       " (146, 7, 122): 'fail',\n",
       " (148, 0, 11): 'got',\n",
       " (148, 1, 23): 'plume',\n",
       " (148, 2, 38): 'call',\n",
       " (148, 7, 129): 'assumed',\n",
       " (148, 8, 162): 'charmed',\n",
       " (148, 9, 176): 'had',\n",
       " (148, 14, 269): 'was',\n",
       " (149, 0, 4): 'walked',\n",
       " (149, 1, 23): 'thought',\n",
       " (149, 2, 62): 'kept',\n",
       " (149, 3, 78): 'saw',\n",
       " (150, 0, 10): 'said',\n",
       " (152, 0, 3): 'putting',\n",
       " (152, 1, 30): 'bore',\n",
       " (152, 2, 60): 'had',\n",
       " (152, 4, 86): 'passed',\n",
       " (153, 0, 6): 'was',\n",
       " (153, 1, 17): 'stood',\n",
       " (153, 4, 36): 'tried',\n",
       " (153, 7, 73): 'was',\n",
       " (153, 8, 131): 'occupied',\n",
       " (155, 0, 2): 'thunderstruck',\n",
       " (155, 1, 9): 'stood',\n",
       " (156, 1, 6): 'murmured',\n",
       " (156, 2, 43): 'went',\n",
       " (156, 3, 101): 'was',\n",
       " (156, 4, 122): 'think',\n",
       " (156, 6, 140): 'was',\n",
       " (156, 7, 163): 'assumed',\n",
       " (156, 8, 192): 'enter',\n",
       " (156, 12, 271): 'resolved',\n",
       " (157, 0, 4): 'said',\n",
       " (157, 2, 33): 'thought',\n",
       " (157, 3, 41): 'imagined',\n",
       " (157, 5, 80): 'added',\n",
       " (157, 6, 90): 'touched',\n",
       " (160, 0, 13): 'replied',\n",
       " (165, 0, 1): 'was',\n",
       " (165, 2, 74): 'was',\n",
       " (167, 0, 1): 'endeavored',\n",
       " (167, 1, 20): 'tried',\n",
       " (167, 7, 149): 'left',\n",
       " (168, 0, 3): 'passed',\n",
       " (168, 2, 45): 'slid',\n",
       " (168, 4, 133): 'feel',\n",
       " (168, 5, 148): 'see',\n",
       " (168, 6, 156): 'penetrate',\n",
       " (168, 7, 167): 'am',\n",
       " (169, 0, 1): 'believe',\n",
       " (169, 2, 77): 'was',\n",
       " (171, 0, 16): 'suggested',\n",
       " (171, 1, 35): 'commended',\n",
       " (172, 0, 3): 'do',\n",
       " (172, 1, 8): 'said',\n",
       " (172, 2, 25): 'do',\n",
       " (172, 3, 28): 'ought',\n",
       " (172, 4, 36): 'say',\n",
       " (172, 6, 62): 'shall',\n",
       " (172, 10, 113): 'do',\n",
       " (172, 11, 120): 'let',\n",
       " (173, 8, 111): 'have',\n",
       " (173, 11, 164): 'quit',\n",
       " (173, 12, 176): 'move',\n",
       " (174, 0, 7): 'addressed',\n",
       " (174, 1, 25): 'is',\n",
       " (174, 2, 34): 'propose',\n",
       " (174, 3, 52): 'tell',\n",
       " (176, 0, 24): 'removed',\n",
       " (176, 1, 30): 'Throughout',\n",
       " (176, 3, 76): 'stood',\n",
       " (177, 0, 3): 'entered',\n",
       " (178, 0, 9): 'going',\n",
       " (178, 1, 36): 'dropped',\n",
       " (180, 0, 1): 'thought',\n",
       " (181, 0, 5): 'replied',\n",
       " (183, 0, 9): 'said',\n",
       " (183, 1, 37): 'is',\n",
       " (185, 0, 5): 'inform',\n",
       " (185, 1, 10): 'know',\n",
       " (185, 2, 18): 'employed',\n",
       " (187, 0, 2): 'passed',\n",
       " (189, 0, 11): 'cried',\n",
       " (191, 0, 6): 'fell',\n",
       " (191, 1, 26): 'persisted',\n",
       " (191, 4, 86): 'considered',\n",
       " (196, 0, 8): 'are',\n",
       " (201, 1, 15): 'like',\n",
       " (203, 0, 12): 'rejoined',\n",
       " (205, 0, 17): 'am',\n",
       " (211, 0, 9): 'cried',\n",
       " (211, 1, 50): 'feel',\n",
       " (211, 3, 75): 'concluded',\n",
       " (211, 4, 101): 'leaving',\n",
       " (212, 0, 4): 'said',\n",
       " (214, 0, 1): 'answered',\n",
       " (214, 1, 51): 'perceived',\n",
       " (214, 2, 102): 'strove',\n",
       " (214, 3, 124): 'was',\n",
       " (214, 4, 137): 'was',\n",
       " (214, 5, 205): 'lived',\n",
       " (215, 0, 14): 'lay',\n",
       " (215, 1, 21): 'opened',\n",
       " (215, 3, 66): 'wished',\n",
       " (215, 5, 97): 'was',\n",
       " (215, 6, 115): 'led',\n",
       " (218, 0, 4): 'received',\n",
       " (218, 1, 31): 'stated',\n",
       " (218, 2, 53): 'assured',\n",
       " (218, 3, 76): 'narrated',\n",
       " (218, 5, 136): 'begged',\n",
       " (219, 1, 42): 'found',\n",
       " (221, 0, 13): 'want',\n",
       " (222, 0, 13): 'said',\n",
       " (223, 0, 9): 'replied',\n",
       " (224, 0, 18): 'accosted',\n",
       " (228, 0, 2): 'am',\n",
       " (231, 0, 5): 'said',\n",
       " (231, 1, 40): 'let',\n",
       " (236, 0, 11): 'said',\n",
       " (236, 1, 26): 'am',\n",
       " (236, 2, 34): 'saying',\n",
       " (238, 0, 10): 'said',\n",
       " (240, 1, 17): 'stop',\n",
       " (240, 4, 38): 'see'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.characters['i'].agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
